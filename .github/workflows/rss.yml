name: Categorized RSS Alerts with Sentiment (Local Symbols)

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "*/10 * * * *"

concurrency:
  group: rss-alerts
  cancel-in-progress: false

jobs:
  notify:
    runs-on: ubuntu-latest

    steps:
      - name: Set up environment
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-utils w3m jq curl coreutils gawk

      # Pull repo contents (so we can read data/symbols.csv)
      - name: Checkout
        uses: actions/checkout@v4

      # Cache dedupe & the derived symbol DB
      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: .cache
          key: rss-cache-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            rss-cache-${{ runner.os }}-

      - name: Ensure cache files
        run: |
          mkdir -p .cache
          [ -f .cache/sent_hashes.txt ] || touch .cache/sent_hashes.txt
          [ -f .cache/last_10min_articles.txt ] || touch .cache/last_10min_articles.txt
          [ -f .cache/symbol_db.json ] || echo '{}' > .cache/symbol_db.json

      - name: Build symbol database from local CSV
        shell: bash
        run: |
          set -euo pipefail
          export LC_ALL=C
          mkdir -p .cache
      
          INPUT_FILE="data/symbols.csv"
          if [ ! -f "$INPUT_FILE" ]; then
            echo "‚ùå $INPUT_FILE not found. Add it in your repo (via GitHub UI: data/symbols.csv)."
            exit 1
          fi
      
          # --- Parse CSV safely with gawk (handles quoted commas) ---
          # Produce TSV lines: "<normalized_key>\t<TICKER>" including alternates (first 2 words, first word)
          gawk -v FPAT='([^,]*)|(\"([^\"]|\"\")+\")' '
            BEGIN{ IGNORECASE=1 }
            NR==1 {
              for(i=1;i<=NF;i++){
                h=$i; gsub(/^"|"$/,"",h); gsub(/""/,"\"",h)
                tl=tolower(h)
                if(tl=="symbol") isym=i
                if(tl=="name")   iname=i
              }
              next
            }
            {
              sym=$isym; name=$iname
              gsub(/^"|"$/,"",sym);  gsub(/""/,"\"",sym)
              gsub(/^"|"$/,"",name); gsub(/""/,"\"",name)
              if(sym=="" || name=="") next
      
              # normalize function inline
              function norm(s, t){
                t=s
                gsub(/[‚Ñ¢¬Æ¬©]/,"",t)
                gsub(/[‚Äú‚Äù"]|\(/,"",t); gsub(/\)/,"",t)
                gsub(/&/," and ",t)
                gsub(/\b(Inc|Incorporated|Corp|Corporation|Co|Company|Ltd|Limited|PLC|Group|Holdings?|LLC|LP|AG|S\.?A\.?)\b\.?,?/,"",t)
                gsub(/[^[:alnum:]_. -]/," ",t)
                gsub(/[ ]+/," ",t); sub(/^[ ]+/,"",t); sub(/[ ]+$/,"",t)
                return tolower(t)
              }
      
              key = norm(name)
              if(key!="") print key "\t" sym
      
              clean=name
              gsub(/[^[:alnum:]_. -]/," ",clean)
              gsub(/[ ]+/," ",clean); sub(/^[ ]+/,"",clean); sub(/[ ]+$/,"",clean)
              n=split(clean, a, /[ ]+/)
      
              if(n>=2){
                alt1 = norm(a[1] " " a[2])
                if(alt1!="") print alt1 "\t" sym
              }
              if(n>=1){
                alt2 = norm(a[1])
                if(alt2!="") print alt2 "\t" sym
              }
            }
          ' "$INPUT_FILE" > /tmp/key_ticker.tsv
      
          # Convert TSV ‚Üí JSONL safely (proper escaping)
          JSONL=/tmp/sym_records.jsonl
          : > "$JSONL"
          while IFS=$'\t' read -r K T; do
            [ -z "${K:-}" ] && continue
            printf '{"k":%s,"t":%s}\n' \
              "$(jq -Rs . <<<"$K")" \
              "$(jq -Rs . <<<"$T")" >> "$JSONL"
          done < /tmp/key_ticker.tsv
      
          # Reduce to symbol_db.json (first win)
          jq -s 'reduce .[] as $r ({}; if has($r.k) then . else .[$r.k] = {ticker:$r.t} end)' "$JSONL" \
            > .cache/symbol_db.json
      
          echo "‚úÖ Built symbol DB with $(jq 'keys|length' .cache/symbol_db.json) entries"


      # ===== Parse feeds + PRN filter (using your local symbol DB) =====
      - name: Parse RSS Feeds, filter PRN via local DB, and Post to Slack
        env:
          SLACK_WEBHOOK_GENERAL: ${{ secrets.SLACK_WEBHOOK_GENERAL }}
          SLACK_WEBHOOK_MEDICAL: ${{ secrets.SLACK_WEBHOOK_MEDICAL }}
          SLACK_WEBHOOK_CONTRACTS: ${{ secrets.SLACK_WEBHOOK_CONTRACTS }}
        run: |
          set -euo pipefail
          export LC_ALL=C

          # Helpers
          safe_xpath() { xmllint --xpath "string($2)" "$1" 2>/dev/null || true; }

          FEEDS=(
            "https://feed.businesswire.com/rss/home/?rss=G1QFDERJXkJeEFpRXEMGSQ5SVFJUGExaFEhaUlJDFUkQUhFUUFNdGEU="
            "https://www.globenewswire.com/rssfeed/exchange/Nasdaq,NYSE"
            "https://www.prnewswire.com/rss/all-news-releases-from-PR-newswire-news.rss"
          )
          PRN_FEED="https://www.prnewswire.com/rss/all-news-releases-from-PR-newswire-news.rss"

          declare -A WEBHOOKS=(
            [general]="${SLACK_WEBHOOK_GENERAL}"
            [medical]="${SLACK_WEBHOOK_MEDICAL}"
            [contracts]="${SLACK_WEBHOOK_CONTRACTS}"
          )

          PER_FEED_ITEMS=3
          WINDOW_SEC=600

          CURRENT_TIME=$(date -u +%s)
          TEN_MINUTES_AGO=$((CURRENT_TIME - WINDOW_SEC))

          # prune rolling 10-min cache ("<epoch> <headline_hash>")
          awk -v cutoff="$TEN_MINUTES_AGO" '{
            if (NF >= 2) { epoch=$1; if (epoch ~ /^[0-9]+$/ && epoch >= cutoff) print $0; }
          }' .cache/last_10min_articles.txt > .cache/_pruned.tmp || true
          mv .cache/_pruned.tmp .cache/last_10min_articles.txt

          norm_company_name() {
            echo "$1" \
              | sed 's/[‚Ñ¢¬Æ¬©]//g' \
              | sed 's/[‚Äú‚Äù"'\''()]//g' \
              | sed 's/&/ and /g' \
              | sed -E 's/\b(Inc|Incorporated|Corp|Corporation|Co|Company|Ltd|Limited|PLC|Group|Holdings?|LLC|LP|AG|S\.?A\.?)\b\.?,?//Ig' \
              | sed -E 's/[^[:alnum:]_. -]/ /g' \
              | tr '[:upper:]' '[:lower:]' \
              | sed -E 's/ +/ /g; s/^ +| +$//g'
          }

          extract_candidates() {
            local title="$1"
            local pre; pre=$(echo "$title" | sed -E 's/ ‚Äî / - /g' | awk -F' - |:|\\|\\|' '{print $1}')
            [ -z "$pre" ] && pre="$title"
            pre=$(echo "$pre" | sed 's/[‚Äú‚Äù"()‚Ñ¢¬Æ¬©]//g' | tr -s ' ')
            local first; first=$(awk '{print $1}' <<<"$pre")
            if echo "$first" | grep -qE '^[A-Z0-9\-]{2,}$'; then echo "$first"; return 0; fi
            VERBS='announces|announce|launches|launch|reports|report|unveils|introduces|reveals|confirms|completes|signs|wins|awarded|enters|partners|acquires|files|prices|lists|successfully|passes|secures|appoints|names|extends|expands|approves|approved'
            awk -v verbs="$VERBS" 'function tolow(s){ out=s; gsub(/[[:punct:]]/,"",out); return tolower(out) }
              BEGIN{ split(verbs,v,"|"); for(i in v) V[v[i]]=1; }
              { n=NF; chunk="";
                for(i=1;i<=n;i++){ wl=tolow($i); if (wl in V) break; if ($i ~ /^[A-Z][A-Za-z0-9.-]*$/ || $i ~ /^[A-Z0-9-]{2,}$/){ chunk = (length(chunk)?chunk" ":"") $i } else if (length(chunk)>0) break }
                print chunk }' <<<"$pre"
          }

          resolve_company_local() {
            local raw="$1"
            local key=$(norm_company_name "$raw")
            local hit
            hit=$(jq -cr --arg k "$key" 'if has($k) then .[$k] else null end' .cache/symbol_db.json)
            if [ "$hit" != "null" ] && [ -n "$hit" ]; then echo "$hit"; return 0; fi
            # progressive shortening (drop last token)
            local words=($key)
            while [ ${#words[@]} -gt 1 ]; do
              unset 'words[${#words[@]}-1]'
              k2="${words[*]}"; k2=$(echo "$k2" | sed -E 's/ +/ /g')
              hit=$(jq -cr --arg k "$k2" 'if has($k) then .[$k] else null end' .cache/symbol_db.json)
              if [ "$hit" != "null" ] && [ -n "$hit" ]; then echo "$hit"; return 0; fi
            done
            echo '{"ticker":null}'
          }

          NEW_ARTICLE_FOUND=false

          for FEED_URL in "${FEEDS[@]}"; do
            echo "üîç Checking Feed: $FEED_URL"

            XML_FILE=$(mktemp)
            curl -sL "$FEED_URL" > "$XML_FILE" || true
            if [ ! -s "$XML_FILE" ]; then
              echo "‚ö†Ô∏è Empty response."
              rm -f "$XML_FILE"
              continue
            fi

            for i in $(seq 1 "$PER_FEED_ITEMS"); do
              LINK=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/link | (//item|//entry)[$i]/link/@href")
              TITLE=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/title")
              DESCRIPTION=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/description | //entry[$i]/summary")
              PUBDATE=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/pubDate | //entry[$i]/updated")

              [ -z "$LINK" ] && continue
              [ -z "$TITLE" ] && TITLE="(No Title)"
              [ -z "$DESCRIPTION" ] && DESCRIPTION="(No summary available)"
              [ -z "$PUBDATE" ] && PUBDATE=""

              DESCRIPTION=$(echo "$DESCRIPTION" | w3m -dump -T text/html | tr -s '[:space:]' ' ' | sed 's/^ *//;s/ *$//' | head -c 300)
              CONTENT="$TITLE $DESCRIPTION"
              CONTENT_LOWER=$(echo "$CONTENT" | tr '[:upper:]' '[:lower:]')

              ARTICLE_TIME=$(date -d "$PUBDATE" +%s 2>/dev/null || echo 0)
              if [ "$ARTICLE_TIME" -eq 0 ] || [ "$ARTICLE_TIME" -lt "$TEN_MINUTES_AGO" ]; then
                echo "‚è±Ô∏è Skipping (invalid/old date): $TITLE"
                continue
              fi

              HEADLINE_HASH=$(echo -n "$TITLE" | md5sum | cut -d ' ' -f1)
              if grep -qx "$HEADLINE_HASH" <(awk '{print $2}' .cache/last_10min_articles.txt); then
                echo "üîÅ Skipping duplicate (10-min window): $TITLE"
                continue
              fi

              LINK_HASH=$(echo -n "$LINK" | md5sum | cut -d ' ' -f1)
              if grep -qx "$LINK_HASH" .cache/sent_hashes.txt; then
                echo "üß≠ Already sent (by link): $TITLE"
                continue
              fi
              if grep -qx "$HEADLINE_HASH" .cache/sent_hashes.txt; then
                echo "üß≠ Already sent (by headline): $TITLE"
                continue
              fi

              # PR Newswire filter: require company match in local DB
              if [ "$FEED_URL" = "$PRN_FEED" ]; then
                cand=$(extract_candidates "$TITLE" | sed -E 's/^ +| +$//g' | head -n1)
                if [ -z "$cand" ]; then
                  echo "üá∫üá∏‚õî PRN skip (no company candidate): $TITLE"
                  continue
                fi
                info_json=$(resolve_company_local "$cand")
                ticker=$(jq -r '.ticker // empty' <<<"$info_json")
                if [ -z "$ticker" ]; then
                  echo "üá∫üá∏‚õî PRN skip (not in local US list): $TITLE (cand: $cand)"
                  continue
                fi
                echo "üá∫üá∏‚úÖ PRN US-listed: $TITLE (cand: $cand, $ticker)"
              fi

              # Categorize
              if echo "$CONTENT_LOWER" | grep -qE "positive topline|positive trial results|collaboration|funding"; then
                CATEGORY="medical"
              elif echo "$CONTENT_LOWER" | grep -qE "contract awarded|acquisition|investment|deal|strategic partnership"; then
                CATEGORY="contracts"
              else
                CATEGORY="general"
              fi

              # Sentiment + naive recommendation
              if echo "$CONTENT_LOWER" | grep -qE "record|growth|surge|positive|expands|secured|approved|funding|investment|invests|acquire|acquired|launch|topline|announces|partnership"; then
                SENTIMENT=":large_green_circle: [POSITIVE]"
                RECOMMENDATION="BUY"
              elif echo "$CONTENT_LOWER" | grep -qE "layoff|lawsuit|delay|loss|drop|decline|recall|fails|resigns|investigation"; then
                SENTIMENT=":red_circle: [NEGATIVE]"
                RECOMMENDATION="SELL"
              else
                SENTIMENT=":white_circle: [NEUTRAL]"
                RECOMMENDATION="HOLD"
              fi

              PAYLOAD=$(jq -n \
                --arg title "$TITLE" \
                --arg desc "$DESCRIPTION" \
                --arg link "$LINK" \
                --arg sentiment "$SENTIMENT" \
                --arg pubdate "${PUBDATE:-Unknown}" \
                --arg rec "$RECOMMENDATION" \
                '{
                  blocks: [
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":bell: *New Article*\n\($sentiment) *\($title)*" } },
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":page_facing_up: \($desc)" } },
                    { "type": "context", "elements": [
                        { "type": "mrkdwn", "text": ":link: <\($link)|Read more>" },
                        { "type": "mrkdwn", "text": ":clock3: Posted: \($pubdate)" },
                        { "type": "mrkdwn", "text": ":mag: Suggestion: *\($rec)*" }
                      ]
                    }
                  ]
                }')

              echo "üì§ Sending to Slack ($CATEGORY)‚Ä¶"
              curl -s -X POST "${WEBHOOKS[$CATEGORY]}" \
                -H 'Content-type: application/json' \
                --data "$PAYLOAD" >/dev/null || true

              echo "$LINK_HASH" >> .cache/sent_hashes.txt
              echo "$HEADLINE_HASH" >> .cache/sent_hashes.txt
              echo "$CURRENT_TIME $HEADLINE_HASH" >> .cache/last_10min_articles.txt

              NEW_ARTICLE_FOUND=true
            done

            rm -f "$XML_FILE"
          done

          if [ "$NEW_ARTICLE_FOUND" = false ]; then
            echo "‚úÖ No new articles in the last 10 minutes."
          fi

      # ===== Manual PRN-style headline test against your local DB =====
      - name: Manual headline extractor + local DB lookup test
        run: |
          set -euo pipefail
          export LC_ALL=C

          norm_company_name() {
            echo "$1" \
              | sed 's/[‚Ñ¢¬Æ¬©]//g' \
              | sed 's/[‚Äú‚Äù"'\''()]//g' \
              | sed 's/&/ and /g' \
              | sed -E 's/\b(Inc|Incorporated|Corp|Corporation|Co|Company|Ltd|Limited|PLC|Group|Holdings?|LLC|LP|AG|S\.?A\.?)\b\.?,?//Ig' \
              | sed -E 's/[^[:alnum:]_. -]/ /g' \
              | tr '[:upper:]' '[:lower:]' \
              | sed -E 's/ +/ /g; s/^ +| +$//g'
          }

          extract_candidates() {
            local title="$1"
            local pre; pre=$(echo "$title" | sed -E 's/ ‚Äî / - /g' | awk -F' - |:|\\|\\|' '{print $1}')
            [ -z "$pre" ] && pre="$title"
            pre=$(echo "$pre" | sed 's/[‚Äú‚Äù"()‚Ñ¢¬Æ¬©]//g' | tr -s ' ')
            local first; first=$(awk '{print $1}' <<<"$pre")
            if echo "$first" | grep -qE '^[A-Z0-9\-]{2,}$'; then echo "$first"; return 0; fi
            VERBS='announces|announce|launches|launch|reports|report|unveils|introduces|reveals|confirms|completes|signs|wins|awarded|enters|partners|acquires|files|prices|lists|successfully|passes|secures|appoints|names|extends|expands|approves|approved'
            awk -v verbs="$VERBS" 'function tolow(s){ out=s; gsub(/[[:punct:]]/,"",out); return tolower(out) }
              BEGIN{ split(verbs,v,"|"); for(i in v) V[v[i]]=1; }
              { n=NF; chunk="";
                for(i=1;i<=n;i++){ wl=tolow($i); if (wl in V) break; if ($i ~ /^[A-Z][A-Za-z0-9.-]*$/ || $i ~ /^[A-Z0-9-]{2,}$/){ chunk = (length(chunk)?chunk" ":"") $i } else if (length(chunk)>0) break }
                print chunk }' <<<"$pre"
          }

          resolve_company_local() {
            local raw="$1"
            local key=$(norm_company_name "$raw")
            local hit
            hit=$(jq -cr --arg k "$key" 'if has($k) then .[$k] else null end' .cache/symbol_db.json)
            if [ "$hit" != "null" ] && [ -n "$hit" ]; then echo "$hit"; return 0; fi
            local words=($key)
            while [ ${#words[@]} -gt 1 ]; do
              unset 'words[${#words[@]}-1]'
              k2="${words[*]}"; k2=$(echo "$k2" | sed -E 's/ +/ /g')
              hit=$(jq -cr --arg k "$k2" 'if has($k) then .[$k] else null end' .cache/symbol_db.json)
              if [ "$hit" != "null" ] && [ -n "$hit" ]; then echo "$hit"; return 0; fi
            done
            echo '{"ticker":null}'
          }

          echo "üì∞ Testing extractor + local DB‚Ä¶"
          HEADLINES=(
            "AEWIN Launches SCB-1953 Series High-Performance Network Appliances Powered by Intel Xeon 6 Processors"
            "Sungrow Inverters Successfully Pass the Brazilian Grid Operator Validations"
            "Apple Announces New AI-Powered iPhone Features"
            "Tesla Signs Strategic Partnership to Expand EV Charging Network"
            "Novartis Reports Positive Topline Results from Phase 3 Trial"
            "Infosys Partners with US Bank for Cloud Modernization"
            "Zoom Video Communications Unveils Next-Gen Collaboration Platform"
            "Pfizer and BioNTech Announce FDA Approval of Updated COVID-19 Vaccine"
          )

          for H in "${HEADLINES[@]}"; do
            cand=$(extract_candidates "$H" | sed -E 's/^ +| +$//g' | head -n1)
            echo "üì∞ $H"
            echo "   ‚Üí Candidate: ${cand:-<none>}"
            if [ -z "${cand:-}" ]; then echo "   ‚Üí ‚ùå no candidate"; echo "---------------------------------"; continue; fi
            info_json=$(resolve_company_local "$cand")
            ticker=$(jq -r '.ticker // empty' <<<"$info_json")
            if [ -n "$ticker" ]; then
              echo "   ‚Üí ‚úÖ $ticker (in your local US list)"
            else
              echo "   ‚Üí ‚ùå not found in your local list"
            fi
            echo "---------------------------------"
          done

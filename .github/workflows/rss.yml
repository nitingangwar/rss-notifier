name: Categorized RSS Alerts with Sentiment

on:
  workflow_dispatch:  # Trigger manually via GitHub
  # schedule:
  #   - cron: "*/10 * * * *"  # Trigger every 10 minutes (can adjust as needed)

jobs:
  notify:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Set up the environment
      - name: Set up environment
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-utils w3m jq curl

      # Step 2: Download previous hashes (if any)
      - name: Download previous hashes (if any)
        uses: actions/download-artifact@v4
        with:
          name: rss-cache
          path: .cache
        continue-on-error: true  # Continue if artifact is not found

      # Step 3: Create cache files if not exists
      - name: Create cache file if not exists
        run: |
          mkdir -p .cache
          touch .cache/sent_hashes.txt
          touch .cache/last_10min_articles.txt

      # Step 4: Parse RSS Feeds and Post to Slack
      - name: Parse RSS Feeds and Post to Slack
        run: |
          FEEDS=(
            "https://feed.businesswire.com/rss/home/?rss=G1QFDERJXkJeEFpRXEMGSQ5SVFJUGExaFEhaUlJDFUkQUhFUUFNdGEU="
            "https://www.globenewswire.com/rssfeed/exchange/Nasdaq,NYSE"
            "https://www.prnewswire.com/rss/news-releases-list.rss"
          )

          declare -A WEBHOOKS
          WEBHOOKS[general]="${{ secrets.SLACK_WEBHOOK_GENERAL }}"
          WEBHOOKS[medical]="${{ secrets.SLACK_WEBHOOK_MEDICAL }}"
          WEBHOOKS[contracts]="${{ secrets.SLACK_WEBHOOK_CONTRACTS }}"

          # Get the current time and calculate the cutoff time for the last 10 minutes
          CURRENT_TIME=$(date +%s)
          TEN_MINUTES_AGO=$((CURRENT_TIME - 600))  # 10 minutes

          NEW_ARTICLE_FOUND=false

          for FEED_URL in "${FEEDS[@]}"; do
            echo "üîç Checking Feed: $FEED_URL"
            XML=$(curl -s "$FEED_URL")

            # Check the first 3 entries per feed (adjust if you want more)
            for i in 1 2 3; do
              LINK=$(echo "$XML" | xmllint --xpath "string((//item|//entry)[$i]/link | (//item|//entry)[$i]/link/@href)" - 2>/dev/null)
              TITLE=$(echo "$XML" | xmllint --xpath "string((//item|//entry)[$i]/title)" - 2>/dev/null)
              DESCRIPTION=$(echo "$XML" | xmllint --xpath "string((//item|//entry)[$i]/description | //entry[$i]/summary)" - 2>/dev/null)
              PUBDATE=$(echo "$XML" | xmllint --xpath "string((//item|//entry)[$i]/pubDate | //entry[$i]/updated)" - 2>/dev/null)

              [ -z "$LINK" ] && continue
              [ -z "$TITLE" ] && TITLE="(No Title)"
              [ -z "$DESCRIPTION" ] && DESCRIPTION="(No summary available)"
              [ -z "$PUBDATE" ] && PUBDATE="(No date provided)"

              DESCRIPTION=$(echo "$DESCRIPTION" | w3m -dump -T text/html | head -c 300)
              CONTENT="$TITLE $DESCRIPTION"
              CONTENT_LOWER=$(echo "$CONTENT" | tr '[:upper:]' '[:lower:]')

              # Time filter (last 10 minutes)
              ARTICLE_TIME=$(date -d "$PUBDATE" +%s 2>/dev/null || echo $CURRENT_TIME)
              if [ "$ARTICLE_TIME" -lt "$TEN_MINUTES_AGO" ]; then
                echo "‚è±Ô∏è Skipping: $TITLE (older than 10 minutes)"
                continue
              fi

              # Short-term dedupe (headline) within last 10 minutes file
              HEADLINE_HASH=$(echo "$TITLE" | md5sum | cut -d ' ' -f 1)
              if grep -q "$HEADLINE_HASH" .cache/last_10min_articles.txt; then
                echo "üîÅ Skipping duplicate headline (10-min window): $TITLE"
                continue
              fi

              # Categorize
              if echo "$CONTENT_LOWER" | grep -qE "positive topline|positive trial results|collaboration|funding"; then
                CATEGORY="medical"
              elif echo "$CONTENT_LOWER" | grep -qE "contract awarded|acquisition|investment|deal|strategic partnership"; then
                CATEGORY="contracts"
              else
                CATEGORY="general"
              fi

              # Sentiment + naive recommendation
              if echo "$CONTENT_LOWER" | grep -qE "record|growth|surge|positive|expands|secured|approved|funding|investment|invests|acquire|acquired|launch|topline|announces|partnership"; then
                SENTIMENT=":large_green_circle: [POSITIVE]"
                RECOMMENDATION="BUY"
              elif echo "$CONTENT_LOWER" | grep -qE "layoff|lawsuit|delay|loss|drop|decline|recall|fails|resigns|investigation"; then
                SENTIMENT=":red_circle: [NEGATIVE]"
                RECOMMENDATION="SELL"
              else
                SENTIMENT=":white_circle: [NEUTRAL]"
                RECOMMENDATION="HOLD"
              fi

              # Global dedupe (by LINK) across runs
              HASH=$(echo "$LINK" | md5sum | cut -d ' ' -f 1)
              if grep -q "$HASH" .cache/sent_hashes.txt; then
                echo "üß≠ Already sent (by link): $TITLE"
                continue
              fi

              # Build Slack message
              TEXT=$(printf ":bell: *New Article:*\n%s *%s*\n:page_facing_up: %s\n:link: <%s|Read more>\n:clock3: *Posted:* %s\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ" \
                "$SENTIMENT" "$TITLE" "$DESCRIPTION" "$LINK" "$PUBDATE")

              PAYLOAD=$(jq -n --arg text "$TEXT" \
                '{blocks: [ { type: "section", text: { type: "mrkdwn", text: $text } } ]}')

              echo "üì§ Sending to Slack ($CATEGORY)‚Ä¶"
              curl -s -X POST "${WEBHOOKS[$CATEGORY]}" \
                -H 'Content-type: application/json' \
                --data "$PAYLOAD" >/dev/null || true

              # Mark as sent (by link) and remember headline for 10-min window
              echo "$HASH" >> .cache/sent_hashes.txt
              echo "$HEADLINE_HASH" >> .cache/last_10min_articles.txt

              NEW_ARTICLE_FOUND=true
            done
          done

          # Exit early (so we don't upload artifacts pointlessly) if nothing was new
          if [ "$NEW_ARTICLE_FOUND" = false ]; then
            echo "‚úÖ No new articles in the last 10 minutes."
            exit 0
          fi

      # Step 5: Upload updated hashes (artifact)
      - name: Upload updated hashes
        uses: actions/upload-artifact@v4
        with:
          name: rss-cache
          path: .cache/sent_hashes.txt

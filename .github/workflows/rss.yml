name: Categorized RSS Alerts with Sentiment (Local TSV + Neg Cache)

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "*/10 * * * *"

concurrency:
  group: rss-alerts
  cancel-in-progress: false

jobs:
  notify:
    runs-on: ubuntu-latest

    steps:
      - name: Set up environment
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-utils w3m jq curl coreutils

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: .cache
          key: rss-cache-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            rss-cache-${{ runner.os }}-

      - name: Ensure cache files
        run: |
          mkdir -p .cache
          [ -f .cache/sent_hashes.txt ] || : > .cache/sent_hashes.txt
          [ -f .cache/last_10min_articles.txt ] || : > .cache/last_10min_articles.txt
          [ -f .cache/symbol_db.json ] || echo '{}' > .cache/symbol_db.json
          [ -f .cache/symbols.sha256 ]   || : > .cache/symbols.sha256
          [ -f .cache/negative_prn.json ] || echo '{}' > .cache/negative_prn.json

      # Build a key‚Üíticker lookup from your tab-separated data/symbols.csv
      - name: Build symbol lookup (only if TSV changed)
        shell: bash
        run: |
          set -euo pipefail
          INPUT_FILE="data/symbols.csv"   # tab-separated file with header
          if [ ! -f "$INPUT_FILE" ]; then
            echo "‚ùå Missing $INPUT_FILE"
            exit 1
          fi

          NEW_HASH="$(sha256sum "$INPUT_FILE" | awk '{print $1}')"
          OLD_HASH="$(cat .cache/symbols.sha256 || true)"

          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -s .cache/symbol_db.json ]; then
            echo "üü¢ symbols.csv unchanged; reusing lookup (keys: $(jq 'keys|length' .cache/symbol_db.json))"
            exit 0
          fi

          echo "üîÑ symbols.csv changed; rebuilding lookup‚Ä¶"
          KT=/tmp/key_ticker.tsv
          : > "$KT"

          norm() {
            local s="${1:-}"
            s="$(printf '%s' "$s" \
              | sed 's/[‚Ñ¢¬Æ¬©]//g' \
              | sed 's/[‚Äú‚Äù"()]//g' \
              | sed 's/&/ and /g' \
              | sed -E 's/\b(Inc|Incorporated|Corp|Corporation|Co|Company|Ltd|Limited|PLC|Group|Holdings?|LLC|LP|AG|S\.?A\.?)\b\.?,?//Ig' \
              | sed -E 's/[^0-9A-Za-z_. -]/ /g' \
              | tr '[:upper:]' '[:lower:]' \
              | sed -E 's/ +/ /g; s/^ +| +$//g')"
            printf '%s' "$s"
          }

          tail -n +2 "$INPUT_FILE" | while IFS=$'\t' read -r sym name _; do
            [ -z "${sym:-}" ] && continue
            [ -z "${name:-}" ] && continue

            key="$(norm "$name")"
            [ -n "$key" ] && printf '%s\t%s\n' "$key" "$sym" >> "$KT"

            words=($name)
            w1="${words[0]:-}"; w2="${words[1]:-}"
            if [ -n "$w1" ]; then
              alt2="$(norm "$w1")"
              [ -n "$alt2" ] && printf '%s\t%s\n' "$alt2" "$sym" >> "$KT"
            fi
            if [ -n "$w1" ] && [ -n "$w2" ]; then
              alt1="$(norm "$w1 $w2")"
              [ -n "$alt1" ] && printf '%s\t%s\n' "$alt1" "$sym" >> "$KT"
            fi
          done

          jq -Rs '
            split("\n")
            | map(select(length>0) | split("\t") | {k:.[0], t:.[1]})
            | reduce .[] as $r ({}; if has($r.k) then . else .[$r.k] = {ticker:$r.t} end)
          ' "$KT" > .cache/symbol_db.json

          echo "$NEW_HASH" > .cache/symbols.sha256
          echo "‚úÖ Lookup built (keys: $(jq 'keys|length' .cache/symbol_db.json))"

      - name: Parse RSS Feeds ‚Üí Slack (with neg cache)
        env:
          SLACK_WEBHOOK_GENERAL: ${{ secrets.SLACK_WEBHOOK_GENERAL }}
          SLACK_WEBHOOK_MEDICAL: ${{ secrets.SLACK_WEBHOOK_MEDICAL }}
          SLACK_WEBHOOK_CONTRACTS: ${{ secrets.SLACK_WEBHOOK_CONTRACTS }}
        shell: bash
        run: |
          set -euo pipefail

          safe_xpath() { xmllint --xpath "string($2)" "$1" 2>/dev/null || true; }

          FEEDS=(
            "https://feed.businesswire.com/rss/home/?rss=G1QFDERJXkJeEFpRXEMGSQ5SVFJUGExaFEhaUlJDFUkQUhFUUFNdGEU="
            "https://www.globenewswire.com/rssfeed/exchange/Nasdaq,NYSE"
            "https://www.prnewswire.com/rss/all-news-releases-from-PR-newswire-news.rss"
          )
          PRN_FEED="https://www.prnewswire.com/rss/all-news-releases-from-PR-newswire-news.rss"

          declare -A WEBHOOKS=(
            [general]="${SLACK_WEBHOOK_GENERAL}"
            [medical]="${SLACK_WEBHOOK_MEDICAL}"
            [contracts]="${SLACK_WEBHOOK_CONTRACTS}"
          )

          NEG_CACHE=".cache/negative_prn.json"

          is_negative_cached() {
            local key="$1"
            jq -e --arg k "$key" 'has($k)' "$NEG_CACHE" >/dev/null 2>&1
          }

          mark_negative() {
            local key="$1"
            tmp=$(mktemp)
            jq --arg k "$key" --arg t "$(date -u +%FT%TZ)" \
               '.[$k] = {last_checked:$t}' "$NEG_CACHE" > "$tmp" && mv "$tmp" "$NEG_CACHE"
          }

          PER_FEED_ITEMS=3
          WINDOW_SEC=600
          CURRENT_TIME=$(date -u +%s)
          TEN_MINUTES_AGO=$((CURRENT_TIME - WINDOW_SEC))

          awk -v cutoff="$TEN_MINUTES_AGO" '{
            if (NF >= 2) { epoch=$1; if (epoch ~ /^[0-9]+$/ && epoch >= cutoff) print $0; }
          }' .cache/last_10min_articles.txt > .cache/_pruned.tmp || true
          mv .cache/_pruned.tmp .cache/last_10min_articles.txt

          norm_company_name() {
            echo "$1" \
              | sed 's/[‚Ñ¢¬Æ¬©]//g' \
              | sed 's/[‚Äú‚Äù"'\''()]//g' \
              | sed 's/&/ and /g' \
              | sed -E 's/\b(Inc|Incorporated|Corp|Corporation|Co|Company|Ltd|Limited|PLC|Group|Holdings?|LLC|LP|AG|S\.?A\.?)\b\.?,?//Ig' \
              | sed -E 's/[^0-9A-Za-z_. -]/ /g' \
              | tr '[:upper:]' '[:lower:]' \
              | sed -E 's/ +/ /g; s/^ +| +$//g'
          }

          extract_firstword() {
            local title="$1"
            local pre; pre=$(echo "$title" | sed -E 's/ ‚Äî / - /g' | awk -F' - |:|\\|\\|' '{print $1}')
            [ -z "$pre" ] && pre="$title"
            local first; first=$(printf '%s' "$pre" | sed 's/[‚Äú‚Äù"()‚Ñ¢¬Æ¬©]//g' | awk '{print $1}')
            echo "$first"
          }

          extract_chunk_before_verb() {
            local title="$1"
            local pre; pre=$(echo "$title" | sed -E 's/ ‚Äî / - /g' | awk -F' - |:|\\|\\|' '{print $1}')
            [ -z "$pre" ] && pre="$title"
            pre=$(echo "$pre" | sed 's/[‚Äú‚Äù"()‚Ñ¢¬Æ¬©]//g' | tr -s ' ')
            awk '{
              n=NF; chunk="";
              verbs="announces announce launches launch reports report unveils introduces reveals confirms completes signs wins awarded enters partners acquires files prices lists successfully passes secures appoints names extends expands approves approved";
              split(verbs,V," ");
              for(i=1;i<=n;i++){
                wl=tolower($i);
                if(wl in V) break;
                if($i ~ /^[A-Z][A-Za-z0-9.-]*$/ || $i ~ /^[A-Z0-9-]{2,}$/){
                  chunk = (length(chunk)?chunk" ":"") $i
                } else if(length(chunk)>0) break
              }
              print chunk
            }' <<<"$pre"
          }

          resolve_company_local() {
            local raw="$1"
            local key=$(norm_company_name "$raw")
            jq -cr --arg k "$key" 'if has($k) then .[$k] else null end' .cache/symbol_db.json
          }

          NEW_ARTICLE_FOUND=false

          for FEED_URL in "${FEEDS[@]}"; do
            echo "üîç Checking Feed: $FEED_URL"
            XML_FILE=$(mktemp)
            curl -sL "$FEED_URL" > "$XML_FILE" || true
            [ ! -s "$XML_FILE" ] && { echo "‚ö†Ô∏è Empty response"; rm -f "$XML_FILE"; continue; }

            for i in $(seq 1 "$PER_FEED_ITEMS"); do
              LINK=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/link | (//item|//entry)[$i]/link/@href")
              TITLE=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/title")
              DESCRIPTION=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/description | //entry[$i]/summary")
              PUBDATE=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/pubDate | //entry[$i]/updated")

              [ -z "$LINK" ] && continue
              [ -z "$TITLE" ] && TITLE="(No Title)"

              DESCRIPTION=$(echo "$DESCRIPTION" | w3m -dump -T text/html | tr -s '[:space:]' ' ' | sed 's/^ *//;s/ *$//' | head -c 300)
              CONTENT="$TITLE $DESCRIPTION"
              CONTENT_LOWER=$(echo "$CONTENT" | tr '[:upper:]' '[:lower:]')

              ARTICLE_TIME=$(date -d "$PUBDATE" +%s 2>/dev/null || echo 0)
              [ "$ARTICLE_TIME" -eq 0 ] && continue
              [ "$ARTICLE_TIME" -lt "$TEN_MINUTES_AGO" ] && continue

              HEADLINE_HASH=$(echo -n "$TITLE" | md5sum | cut -d ' ' -f1)
              if grep -qx "$HEADLINE_HASH" <(awk '{print $2}' .cache/last_10min_articles.txt); then continue; fi

              LINK_HASH=$(echo -n "$LINK" | md5sum | cut -d ' ' -f1)
              if grep -qx "$LINK_HASH" .cache/sent_hashes.txt; then continue; fi

              if [ "$FEED_URL" = "$PRN_FEED" ]; then
                fw=$(extract_firstword "$TITLE")
                cand_key=$(norm_company_name "$fw")

                if is_negative_cached "$cand_key"; then
                  echo "üö´ Skipping PRN (cached negative): $TITLE (fw: $fw)"
                  continue
                fi

                fw_info=$(resolve_company_local "$fw")
                fw_ticker=$(jq -r '.ticker // empty' <<<"$fw_info")

                if [ -n "$fw_ticker" ]; then
                  echo "üá∫üá∏‚úÖ PRN matched by first word: $TITLE ($fw ‚Üí $fw_ticker)"
                else
                  cand=$(extract_chunk_before_verb "$TITLE" | sed -E 's/^ +| +$//g' | head -n1)
                  [ -z "$cand" ] && { mark_negative "$cand_key"; continue; }

                  cand_key=$(norm_company_name "$cand")
                  if is_negative_cached "$cand_key"; then
                    echo "üö´ Skipping PRN (cached negative): $TITLE (cand: $cand)"
                    continue
                  fi

                  info_json=$(resolve_company_local "$cand")
                  ticker=$(jq -r '.ticker // empty' <<<"$info_json")
                  if [ -z "$ticker" ]; then
                    echo "üö´ PRN not US-listed ‚Üí caching negative: $TITLE (cand: $cand)"
                    mark_negative "$cand_key"
                    continue
                  fi
                  echo "üá∫üá∏‚úÖ PRN matched by chunk: $TITLE (cand: $cand ‚Üí $ticker)"
                fi
              fi

              CATEGORY="general"
              if echo "$CONTENT_LOWER" | grep -qE "positive topline|positive trial results|collaboration|funding"; then
                CATEGORY="medical"
              elif echo "$CONTENT_LOWER" | grep -qE "contract awarded|acquisition|investment|deal|strategic partnership"; then
                CATEGORY="contracts"
              fi

              SENTIMENT=":white_circle: [NEUTRAL]"
              RECOMMENDATION="HOLD"
              if echo "$CONTENT_LOWER" | grep -qE "record|growth|surge|positive|expands|secured|approved|funding|investment|invests|acquire|acquired|launch|topline|announces|partnership"; then
                SENTIMENT=":large_green_circle: [POSITIVE]"
                RECOMMENDATION="BUY"
              elif echo "$CONTENT_LOWER" | grep -qE "layoff|lawsuit|delay|loss|drop|decline|recall|fails|resigns|investigation"; then
                SENTIMENT=":red_circle: [NEGATIVE]"
                RECOMMENDATION="SELL"
              fi

              PAYLOAD=$(jq -n \
                --arg title "$TITLE" \
                --arg desc "$DESCRIPTION" \
                --arg link "$LINK" \
                --arg sentiment "$SENTIMENT" \
                --arg pubdate "${PUBDATE:-Unknown}" \
                --arg rec "$RECOMMENDATION" \
                '{
                  blocks: [
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":bell: *New Article*\n\($sentiment) *\($title)*" } },
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":page_facing_up: \($desc)" } },
                    { "type": "context", "elements": [
                        { "type": "mrkdwn", "text": ":link: <\($link)|Read more>" },
                        { "type": "mrkdwn", "text": ":clock3: Posted: \($pubdate)" },
                        { "type": "mrkdwn", "text": ":mag: Suggestion: *\($rec)*" }
                      ]
                    }
                  ]
                }')

              curl -s -X POST "${WEBHOOKS[$CATEGORY]}" \
                -H 'Content-type: application/json' \
                --data "$PAYLOAD" >/dev/null || true

              echo "$LINK_HASH" >> .cache/sent_hashes.txt
              echo "$HEADLINE_HASH" >> .cache/sent_hashes.txt
              echo "$CURRENT_TIME $HEADLINE_HASH" >> .cache/last_10min_articles.txt
              NEW_ARTICLE_FOUND=true
            done
            rm -f "$XML_FILE"
          done

          [ "$NEW_ARTICLE_FOUND" = false ] && echo "‚úÖ No new articles."

name: Categorized RSS Alerts with Sentiment

on:
  workflow_dispatch:
    inputs:
      debug:
        description: "Enable verbose debugging (1=yes, 0=no)"
        default: "0"
        required: false
  # schedule:
  #   - cron: "*/10 * * * *"

concurrency:
  group: rss-alerts
  cancel-in-progress: false

jobs:
  notify:
    runs-on: ubuntu-latest

    steps:
      - name: Ensure .cache dir always exists
        run: mkdir -p .cache

      - name: Set up environment
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-utils w3m jq curl coreutils gawk

      - name: Checkout repo (to read data files)
        uses: actions/checkout@v4

      - name: Week stamp
        id: stamp
        run: echo "week=$(date -u +%G-%V)" >> "$GITHUB_OUTPUT"

      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: .cache
          key: rss-cache-v3-${{ runner.os }}-w${{ steps.stamp.outputs.week }}
          restore-keys: |
            rss-cache-v3-${{ runner.os }}-

      - name: Ensure cache files
        run: |
          mkdir -p .cache
          [ -f .cache/sent_link_hashes.txt ]    || : > .cache/sent_link_hashes.txt
          [ -f .cache/sent_title_hashes.txt ]   || : > .cache/sent_title_hashes.txt
          [ -f .cache/last_10min_articles.txt ] || : > .cache/last_10min_articles.txt
          [ -f .cache/symbol_db.json ]          || echo '{}' > .cache/symbol_db.json
          [ -f .cache/symbols.sha256 ]          || : > .cache/symbols.sha256
          [ -f .cache/negative_prn.json ]       || echo '{}' > .cache/negative_prn.json
          [ -f .cache/important_keys.json ]     || echo '[]' > .cache/important_keys.json
          [ -f .cache/important.sha256 ]        || : > .cache/important.sha256

          NOW=$(date -u +%s)
          CUTOFF=$((NOW - 600))
          awk -v cutoff="$CUTOFF" 'NF>=2 { if ($1 ~ /^[0-9]+$/ && $1 >= cutoff) print $0 }' \
            .cache/last_10min_articles.txt > .cache/_pruned || true
          mv .cache/_pruned .cache/last_10min_articles.txt

      # === Build symbol DB from your uploaded CSV ===
      - name: Build symbol lookup (from symbols.csv)
        run: |
          set -euo pipefail
          INPUT_FILE="data/symbols.csv"
          if [ ! -f "$INPUT_FILE" ]; then
            echo "‚ùå Missing $INPUT_FILE"; echo '{}' > .cache/symbol_db.json; exit 0
          fi
          NEW_HASH="$(sha256sum "$INPUT_FILE" | awk '{print $1}')"
          OLD_HASH="$(cat .cache/symbols.sha256 || true)"
          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -s .cache/symbol_db.json ]; then
            echo "üü¢ symbols.csv unchanged; keys: $(jq 'keys|length' .cache/symbol_db.json)"
          else
            echo "üîÑ Rebuilding symbol_db.json from symbols.csv‚Ä¶"
            tail -n +2 "$INPUT_FILE" | \
              awk -F'\t' 'NF>=2 {sym=$1; name=$2; gsub(/[‚Ñ¢¬Æ¬©]/,"",name); gsub(/[‚Äú‚Äù"()]/,"",name); gsub(/&/," and ",name); sub(/^ +| +$/,"",name); n=split(name,toks," "); for (i=1;i<=4 && i<=n;i++){pref=""; for(j=1;j<=i;j++) pref=(pref?pref" ":"") toks[j]; key=tolower(pref); if(key!=""){print key"\t"sym}} }' | \
              jq -Rn 'reduce (inputs | split("\t")) as $i ({}; .[$i[0]] = {ticker:$i[1]})' > .cache/symbol_db.json
            echo "$NEW_HASH" > .cache/symbols.sha256
            echo "‚úÖ symbol_db.json rebuilt (keys: $(jq 'keys|length' .cache/symbol_db.json))"
          fi

      # === Important companies list ===
      - name: Build important companies key list
        run: |
          set -euo pipefail
          INPUT_FILE="data/important_companies.tsv"
          if [ ! -f "$INPUT_FILE" ]; then
            echo "‚ùå Missing $INPUT_FILE"; echo '[]' > .cache/important_keys.json; exit 0
          fi
          NEW_HASH="$(sha256sum "$INPUT_FILE" | awk '{print $1}')"
          OLD_HASH="$(cat .cache/important.sha256 || true)"
          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -s .cache/important_keys.json ]; then
            echo "üü¢ important_companies unchanged"
          else
            echo "üîÑ Rebuilding important_keys.json"
            tail -n +2 "$INPUT_FILE" | cut -f1 | \
              awk '{print tolower($0)}' | jq -Rsc 'split("\n")|map(select(length>0))' > .cache/important_keys.json
            echo "$NEW_HASH" > .cache/important.sha256
            echo "‚úÖ important_keys.json built: $(jq 'length' .cache/important_keys.json) companies"
          fi

      # === Parse feeds ===
      - name: Parse RSS feeds and post to Slack
        env:
          SLACK_WEBHOOK_GENERAL:   ${{ secrets.SLACK_WEBHOOK_GENERAL }}
          SLACK_WEBHOOK_MEDICAL:   ${{ secrets.SLACK_WEBHOOK_MEDICAL }}
          SLACK_WEBHOOK_CONTRACTS: ${{ secrets.SLACK_WEBHOOK_CONTRACTS }}
          SLACK_WEBHOOK_EARNINGS:  ${{ secrets.SLACK_WEBHOOK_EARNINGS }}
          SLACK_WEBHOOK_IMP:       ${{ secrets.SLACK_WEBHOOK_IMP_COMPANIES }}
          DEBUG:                   ${{ github.event.inputs.debug }}
        run: |
          set -euo pipefail
          echo "üîß DEBUG=$DEBUG"

          FEEDS=(
            "https://feed.businesswire.com/rss/home/?rss=G1QFDERJXkJeEFpRXEMGSQ5SVFJUGExaFEhaUlJDFUkQUhFUUFNdGEU="
            "https://www.globenewswire.com/rssfeed/exchange/Nasdaq,NYSE"
            "https://www.prnewswire.com/rss/news-releases-list.rss"
          )
          PRN_FEED="https://www.prnewswire.com/rss/news-releases-list.rss"

          declare -A WEBHOOKS=(
            [general]="${SLACK_WEBHOOK_GENERAL}"
            [medical]="${SLACK_WEBHOOK_MEDICAL}"
            [contracts]="${SLACK_WEBHOOK_CONTRACTS}"
            [earnings]="${SLACK_WEBHOOK_EARNINGS}"
            [important]="${SLACK_WEBHOOK_IMP}"
          )

          PER_FEED_ITEMS=0
          WINDOW_SEC=600
          CURRENT_TIME=$(date -u +%s)
          TEN_MINUTES_AGO=$((CURRENT_TIME - WINDOW_SEC))

          safe_xpath() { xmllint --xpath "string($2)" "$1" 2>/dev/null || true; }

          for FEED_URL in "${FEEDS[@]}"; do
            echo "üîç Checking Feed: $FEED_URL"
            XML_FILE=$(mktemp)
            curl -sL --max-time 20 "$FEED_URL" > "$XML_FILE" || true
            ITEMS=$(xmllint --xpath 'count(//item | //entry)' "$XML_FILE" 2>/dev/null || echo 0)
            ITEMS=${ITEMS%.*}
            [ "$PER_FEED_ITEMS" -gt 0 ] && MAX=$PER_FEED_ITEMS || MAX=$ITEMS
            echo "üìä Feed items: $ITEMS (processing $MAX)"

            for i in $(seq 1 "$MAX"); do
              LINK=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/link | (//item|//entry)[$i]/link/@href")
              TITLE=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/title")
              DESCRIPTION=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/description | //entry[$i]/summary")
              PUBDATE=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/pubDate | //entry[$i]/updated")

              [ -z "$LINK" ] && continue
              [ -z "$TITLE" ] && TITLE="(No Title)"
              [ -z "$DESCRIPTION" ] && DESCRIPTION="(No summary available)"

              DESCRIPTION=$(echo "$DESCRIPTION" | w3m -dump -T text/html | tr -s '[:space:]' ' ' | head -c 300)
              CONTENT="$TITLE $DESCRIPTION"
              CONTENT_LOWER=$(echo "$CONTENT" | tr '[:upper:]' '[:lower:]')
              ARTICLE_TIME=$(date -d "$PUBDATE" +%s 2>/dev/null || echo 0)

              if [ "$ARTICLE_TIME" -lt "$TEN_MINUTES_AGO" ]; then
                continue
              fi

              HEADLINE_HASH=$(echo -n "$TITLE" | sha256sum | cut -d ' ' -f1)
              LINK_HASH=$(echo -n "$LINK" | sha256sum | cut -d ' ' -f1)
              if grep -qx "$HEADLINE_HASH" .cache/sent_title_hashes.txt || grep -qx "$LINK_HASH" .cache/sent_link_hashes.txt; then
                continue
              fi

              # === Earnings detection (improved regex) ===
              if echo "$CONTENT_LOWER" | grep -qiE "reports (first|second|third|fourth) quarter|reports q[1-4]|quarterly results|fiscal|earnings|full year"; then
                CATEGORY="earnings"
              elif echo "$CONTENT_LOWER" | grep -qE "positive topline|positive trial results|collaboration|funding"; then
                CATEGORY="medical"
              elif echo "$CONTENT_LOWER" | grep -qE "contract awarded|acquisition|investment|deal|strategic partnership"; then
                CATEGORY="contracts"
              else
                CATEGORY="general"
              fi

              LOW_TITLE=$(echo "$TITLE" | tr '[:upper:]' '[:lower:]')
              if jq -r '.[]' .cache/important_keys.json | grep -qF "$LOW_TITLE"; then
                CATEGORY="important"
              fi

              if echo "$CONTENT_LOWER" | grep -qE "record|growth|surge|positive|expands|secured|approved|funding|investment|invests|acquire|acquired|launch|topline|announces|partnership"; then
                SENTIMENT=":large_green_circle: [POSITIVE]"
                RECOMMENDATION="BUY"
              elif echo "$CONTENT_LOWER" | grep -qE "layoff|lawsuit|delay|loss|drop|decline|recall|fails|resigns|investigation"; then
                SENTIMENT=":red_circle: [NEGATIVE]"
                RECOMMENDATION="SELL"
              else
                SENTIMENT=":white_circle: [NEUTRAL]"
                RECOMMENDATION="HOLD"
              fi

              PAYLOAD=$(jq -n \
                --arg title "$TITLE" \
                --arg desc "$DESCRIPTION" \
                --arg link "$LINK" \
                --arg sentiment "$SENTIMENT" \
                --arg pubdate "${PUBDATE:-Unknown}" \
                --arg rec "$RECOMMENDATION" \
                '{
                  blocks: [
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":bell: *New Article*\n\($sentiment) *\($title)*" } },
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":page_facing_up: \($desc)" } },
                    { "type": "context", "elements": [
                        { "type": "mrkdwn", "text": ":link: <\($link)|Read more>" },
                        { "type": "mrkdwn", "text": ":clock3: Posted: \($pubdate)" },
                        { "type": "mrkdwn", "text": ":mag: Suggestion: *\($rec)*" }
                      ]
                    }
                  ]
                }')

              echo "üì§ Sending to Slack ($CATEGORY)‚Ä¶"
              curl -s -X POST "${WEBHOOKS[$CATEGORY]}" \
                -H 'Content-type: application/json' \
                --data "$PAYLOAD" >/dev/null || true

              echo "$LINK_HASH" >> .cache/sent_link_hashes.txt
              echo "$HEADLINE_HASH" >> .cache/sent_title_hashes.txt
              echo "$CURRENT_TIME $HEADLINE_HASH" >> .cache/last_10min_articles.txt
            done
            rm -f "$XML_FILE"
          done

      - name: Upload updated caches
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rss-cache
          path: .cache/**
          if-no-files-found: ignore

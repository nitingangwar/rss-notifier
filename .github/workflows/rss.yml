name: Categorized RSS Alerts with Sentiment

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "*/10 * * * *"

concurrency:
  group: rss-alerts
  cancel-in-progress: false

jobs:
  notify:
    runs-on: ubuntu-latest

    steps:
      - name: Set up environment
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-utils w3m jq curl coreutils

      - name: Checkout repo (to read data/symbols.csv)
        uses: actions/checkout@v4

      # Cache for dedupe + symbol DB + negative cache
      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: .cache
          key: rss-cache-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            rss-cache-${{ runner.os }}-

      - name: Ensure cache files
        run: |
          mkdir -p .cache
          [ -f .cache/sent_hashes.txt ] || : > .cache/sent_hashes.txt
          [ -f .cache/last_10min_articles.txt ] || : > .cache/last_10min_articles.txt
          [ -f .cache/symbol_db.json ] || echo '{}' > .cache/symbol_db.json
          [ -f .cache/symbols.sha256 ]   || : > .cache/symbols.sha256
          [ -f .cache/negative_prn.json ] || echo '{}' > .cache/negative_prn.json

      - name: Reset negative cache (one-time fix)
        run: |
          echo '{}' > .cache/negative_prn.json

      # Build key‚Üíticker map from your TAB-separated data/symbols.csv (uses 1‚Äì4 word prefixes)
      - name: Build symbol lookup (rebuild only if TSV changed)
        shell: bash
        run: |
          set -euo pipefail
          INPUT_FILE="data/symbols.csv"   # TAB-separated; header row; first two cols: Symbol, Name
          if [ ! -f "$INPUT_FILE" ]; then
            echo "‚ùå Missing $INPUT_FILE (tab-separated)."; exit 1
          fi

          NEW_HASH="$(sha256sum "$INPUT_FILE" | awk '{print $1}')"
          OLD_HASH="$(cat .cache/symbols.sha256 || true)"

          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -s .cache/symbol_db.json ]; then
            echo "üü¢ symbols.csv unchanged; reusing lookup (keys: $(jq 'keys|length' .cache/symbol_db.json))"
            exit 0
          fi

          echo "üîÑ Rebuilding lookup from symbols.csv (prefix keys 1..4 words)‚Ä¶"
          KT=/tmp/key_ticker.tsv
          : > "$KT"

          # Read first two columns: Symbol<TAB>Name<‚Ä¶>
          tail -n +2 "$INPUT_FILE" | while IFS=$'\t' read -r sym name _; do
            [ -n "${sym:-}" ] || continue
            [ -n "${name:-}" ] || continue
            # collapse spaces
            name="$(printf '%s' "$name" | tr -s '[:space:]' ' ')"
            # tokenize by space (keep + and &)
            read -r -a TOKS <<<"$name"
            for n in 1 2 3 4; do
              [ ${#TOKS[@]} -ge $n ] || break
              prefix="${TOKS[*]:0:$n}"
              key="$(printf '%s' "$prefix" | tr '[:upper:]' '[:lower:]' | sed -E 's/ +/ /g; s/^ +| +$//g')"
              [ -n "$key" ] && printf '%s\t%s\n' "$key" "$sym" >> "$KT"
              # safety variant: +/& ‚Üí "and"
              key_and="$(printf '%s' "$key" | sed 's/[+&]/ and /g' | sed -E 's/ +/ /g')"
              [ "$key_and" != "$key" ] && printf '%s\t%s\n' "$key_and" "$sym" >> "$KT"
            done
          done

          jq -Rs '
            split("\n")
            | map(select(length>0) | split("\t") | {k:.[0], t:.[1]})
            | reduce .[] as $r ({}; if has($r.k) then . else .[$r.k] = {ticker:$r.t} end)
          ' "$KT" > .cache/symbol_db.json

          echo "$NEW_HASH" > .cache/symbols.sha256
          echo "‚úÖ Lookup built (keys: $(jq 'keys|length' .cache/symbol_db.json))"

      - name: Parse RSS feeds and post to Slack
        env:
          SLACK_WEBHOOK_GENERAL: ${{ secrets.SLACK_WEBHOOK_GENERAL }}
          SLACK_WEBHOOK_MEDICAL: ${{ secrets.SLACK_WEBHOOK_MEDICAL }}
          SLACK_WEBHOOK_CONTRACTS: ${{ secrets.SLACK_WEBHOOK_CONTRACTS }}
        shell: bash
        run: |
          set -euo pipefail

          FEEDS=(
            "https://feed.businesswire.com/rss/home/?rss=G1QFDERJXkJeEFpRXEMGSQ5SVFJUGExaFEhaUlJDFUkQUhFUUFNdGEU="
            "https://www.globenewswire.com/rssfeed/exchange/Nasdaq,NYSE"
            "https://www.prnewswire.com/rss/news-releases-list.rss"
          )
          PRN_FEED="https://www.prnewswire.com/rss/news-releases-list.rss"

          declare -A WEBHOOKS=(
            [general]="${SLACK_WEBHOOK_GENERAL}"
            [medical]="${SLACK_WEBHOOK_MEDICAL}"
            [contracts]="${SLACK_WEBHOOK_CONTRACTS}"
          )

          PER_FEED_ITEMS=3
          WINDOW_SEC=600
          CURRENT_TIME=$(date -u +%s)
          CUTOFF=$((CURRENT_TIME - WINDOW_SEC))

          NEG_CACHE=".cache/negative_prn.json"
          is_negative_cached() {
            local key="$1"
            jq -e --arg k "$key" 'has($k)' "$NEG_CACHE" >/dev/null 2>&1
          }
          mark_negative() {
            local key="$1"
            tmp=$(mktemp)
            jq --arg k "$key" --arg t "$(date -u +%FT%TZ)" \
               '.[$k] = {last_checked:$t}' "$NEG_CACHE" > "$tmp" && mv "$tmp" "$NEG_CACHE"
          }
          unmark_negative() {
            local key="$1"
            tmp=$(mktemp)
            jq --arg k "$key" 'del(.[$k])' "$NEG_CACHE" > "$tmp" && mv "$tmp" "$NEG_CACHE"
          }

          safe_xpath() { xmllint --xpath "string($2)" "$1" 2>/dev/null || true; }

          # super-light resolver: lowercase+trim; try raw, then +/& ‚Üí and
          resolve_company_local() {
            local raw="$1" k hit
            k="$(printf '%s' "$raw" | tr '[:upper:]' '[:lower:]' | sed -E 's/ +/ /g; s/^ +| +$//g')"
            hit=$(jq -cr --arg k "$k" 'if has($k) then .[$k] else null end' .cache/symbol_db.json)
            if [ "$hit" != "null" ] && [ -n "$hit" ]; then echo "$hit"; return 0; fi
            k2="$(printf '%s' "$k" | sed 's/[+&]/ and /g' | sed -E 's/ +/ /g')"
            hit=$(jq -cr --arg k "$k2" 'if has($k) then .[$k] else null end' .cache/symbol_db.json)
            if [ "$hit" != "null" ] && [ -n "$hit" ]; then echo "$hit"; return 0; fi
            echo '{"ticker":null}'
          }

          extract_chunk_before_verb() {
            local title="$1"
            local pre; pre=$(echo "$title" | sed -E 's/ ‚Äî / - /g' | awk -F' - |:|\\|\\|' '{print $1}')
            [ -z "$pre" ] && pre="$title"
            pre=$(echo "$pre" | sed 's/[‚Äú‚Äù"()‚Ñ¢¬Æ¬©]//g' | tr -s ' ')
            awk '{
              n=NF; chunk="";
              verbs="announces announce launches launch reports report unveils introduces reveals confirms completes signs wins awarded enters partners acquires files prices lists successfully passes secures appoints names extends expands approves approved"
              split(verbs,V," ")
              for(i=1;i<=n;i++){
                wl=tolower($i); if(wl in V) break
                if($i ~ /^[A-Z][A-Za-z0-9.+&-]*$/ || $i ~ /^[A-Z0-9-]{2,}$/){
                  chunk = (length(chunk)?chunk" ":"") $i
                } else if(length(chunk)>0) break
              }
              print chunk
            }' <<<"$pre"
          }

          NEW_ARTICLE_FOUND=false

          for FEED_URL in "${FEEDS[@]}"; do
            echo "üîç Checking Feed: $FEED_URL"

            XML_FILE=$(mktemp)
            curl -sL "$FEED_URL" > "$XML_FILE" || true
            if [ ! -s "$XML_FILE" ]; then
              echo "‚ö†Ô∏è Empty response."
              rm -f "$XML_FILE"
              continue
            fi

            ITEMS=$(xmllint --xpath 'count(//item | //entry)' "$XML_FILE" 2>/dev/null || echo 0)
            ITEMS=${ITEMS%.*} 

            for i in $(seq 1 "$ITEMS"); do
              LINK=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/link | (//item|//entry)[$i]/link/@href")
              TITLE=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/title")
              DESCRIPTION=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/description | //entry[$i]/summary")
              PUBDATE=$(safe_xpath "$XML_FILE" "(//item|//entry)[$i]/pubDate | //entry[$i]/updated")

              [ -z "$LINK" ] && continue
              [ -z "$TITLE" ] && TITLE="(No Title)"
              [ -z "$DESCRIPTION" ] && DESCRIPTION="(No summary available)"

              DESCRIPTION=$(echo "$DESCRIPTION" | w3m -dump -T text/html | tr -s '[:space:]' ' ' | sed 's/^ *//;s/ *$//' | head -c 300)
              CONTENT="$TITLE $DESCRIPTION"
              CONTENT_LOWER=$(echo "$CONTENT" | tr '[:upper:]' '[:lower:]')

              ARTICLE_TIME=$(date -d "$PUBDATE" +%s 2>/dev/null || echo 0)
              if [ "$ARTICLE_TIME" -eq 0 ] || [ "$ARTICLE_TIME" -lt "$CUTOFF" ]; then
                echo "‚è±Ô∏è Skipping (outside window): $TITLE"
                continue
              fi

              HEADLINE_HASH=$(echo -n "$TITLE" | md5sum | cut -d ' ' -f1)
              if grep -qx "$HEADLINE_HASH" <(awk '{print $2}' .cache/last_10min_articles.txt); then
                echo "üîÅ Skipping duplicate headline (10-min window): $TITLE"
                continue
              fi
              LINK_HASH=$(echo -n "$LINK" | md5sum | cut -d ' ' -f1)
              if grep -qx "$LINK_HASH" .cache/sent_hashes.txt; then
                echo "üß≠ Already sent (by link): $TITLE"
                continue
              fi
              if grep -qx "$HEADLINE_HASH" .cache/sent_hashes.txt; then
                echo "üß≠ Already sent (by headline): $TITLE"
                continue
              fi

              # ===== PR Newswire filter (prefix-based) =====
              if [ "$FEED_URL" = "https://www.prnewswire.com/rss/news-releases-list.rss" ]; then
                matched=false

                # 1) First word
                fw=$(echo "$TITLE" | awk '{print $1}')
                info=$(resolve_company_local "$fw")
                ticker=$(jq -r '.ticker // empty' <<<"$info")
                if [ -n "$ticker" ]; then
                  matched=true
                  key_fw="$(printf '%s' "$fw" | tr '[:upper:]' '[:lower:]' | sed -E 's/ +/ /g')"
                  unmark_negative "$key_fw" || true
                  echo "üá∫üá∏‚úÖ PRN matched by 1-word prefix: $TITLE ($fw ‚Üí $ticker)"
                fi

                # 2) Fallback: chunk before verb ‚Üí take its first 2 words
                if [ "$matched" = false ]; then
                  cand_full=$(extract_chunk_before_verb "$TITLE" | sed -E 's/^ +| +$//g' | head -n1)
                  if [ -n "$cand_full" ]; then
                    cand2="$(printf '%s' "$cand_full" | awk '{print $1" "$2}')"
                    info2=$(resolve_company_local "$cand2")
                    ticker2=$(jq -r '.ticker // empty' <<<"$info2")
                    if [ -n "$ticker2" ]; then
                      matched=true
                      key_c2="$(printf '%s' "$cand2" | tr '[:upper:]' '[:lower:]' | sed -E 's/ +/ /g')"
                      unmark_negative "$key_c2" || true
                      echo "üá∫üá∏‚úÖ PRN matched by 2-word prefix: $TITLE ($cand2 ‚Üí $ticker2)"
                    fi
                  fi
                fi

                # 3) If still not matched, cache negative to avoid rework
                if [ "$matched" = false ]; then
                  key_neg="$(printf '%s' "${cand_full:-$fw}" | tr '[:upper:]' '[:lower:]' | sed -E 's/ +/ /g')"
                  mark_negative "$key_neg"
                  echo "üö´ PRN not in local US list ‚Üí cached negative: $TITLE (cand: ${cand_full:-$fw})"
                  continue
                fi
              fi
              # ===== end PRN filter =====

              # Category
              if echo "$CONTENT_LOWER" | grep -qE "positive topline|positive trial results|collaboration|funding"; then
                CATEGORY="medical"
              elif echo "$CONTENT_LOWER" | grep -qE "contract awarded|acquisition|investment|deal|strategic partnership"; then
                CATEGORY="contracts"
              else
                CATEGORY="general"
              fi

              # Sentiment
              if echo "$CONTENT_LOWER" | grep -qE "record|growth|surge|positive|expands|secured|approved|funding|investment|invests|acquire|acquired|launch|topline|announces|partnership"; then
                SENTIMENT=":large_green_circle: [POSITIVE]"; RECOMMENDATION="BUY"
              elif echo "$CONTENT_LOWER" | grep -qE "layoff|lawsuit|delay|loss|drop|decline|recall|fails|resigns|investigation"; then
                SENTIMENT=":red_circle: [NEGATIVE]"; RECOMMENDATION="SELL"
              else
                SENTIMENT=":white_circle: [NEUTRAL]"; RECOMMENDATION="HOLD"
              fi

              PAYLOAD=$(jq -n \
                --arg title "$TITLE" \
                --arg desc "$DESCRIPTION" \
                --arg link "$LINK" \
                --arg sentiment "$SENTIMENT" \
                --arg pubdate "${PUBDATE:-Unknown}" \
                --arg rec "$RECOMMENDATION" \
                '{
                  blocks: [
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":bell: *New Article*\n\($sentiment) *\($title)*" } },
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":page_facing_up: \($desc)" } },
                    { "type": "context", "elements": [
                        { "type": "mrkdwn", "text": ":link: <\($link)|Read more>" },
                        { "type": "mrkdwn", "text": ":clock3: Posted: \($pubdate)" },
                        { "type": "mrkdwn", "text": ":mag: Suggestion: *\($rec)*" }
                      ]
                    }
                  ]
                }')

              curl -s -X POST "${WEBHOOKS[$CATEGORY]}" -H 'Content-type: application/json' --data "$PAYLOAD" >/dev/null || true

              echo "$LINK_HASH" >> .cache/sent_hashes.txt
              echo "$HEADLINE_HASH" >> .cache/sent_hashes.txt
              echo "$CURRENT_TIME $HEADLINE_HASH" >> .cache/last_10min_articles.txt
              NEW_ARTICLE_FOUND=true
            done

            rm -f "$XML_FILE"
          done

          if [ "$NEW_ARTICLE_FOUND" = false ]; then
            echo "‚úÖ No new articles in the last 10 minutes."
          fi
          exit 0

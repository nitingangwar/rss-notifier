name: Categorized RSS Alerts with Sentiment

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "*/10 * * * *"

concurrency:
  group: rss-alerts
  cancel-in-progress: false

jobs:
  notify:
    runs-on: ubuntu-latest

    steps:
      # 1) Environment
      - name: Set up environment
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-utils w3m jq curl coreutils

      # 2) Restore/prime cross-run cache (mutable state for dedupe + inventory)
      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: .cache
          key: rss-cache-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            rss-cache-${{ runner.os }}-

      # 3) Ensure cache files exist
      - name: Create cache files if not exists
        run: |
          mkdir -p .cache
          [ -f .cache/sent_hashes.txt ] || touch .cache/sent_hashes.txt
          [ -f .cache/last_10min_articles.txt ] || touch .cache/last_10min_articles.txt
          [ -f .cache/company_index.json ] || echo '{}' > .cache/company_index.json

      # 3a) (Optional) Sanity-check Yahoo endpoint (logs only; non-blocking)
      - name: Sanity-check Yahoo search endpoint
        run: |
          UA='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'
          echo "ðŸ”Ž Yahoo Finance search sanity check (Apple):"
          curl -sL --max-time 6 -H "User-Agent: $UA" \
            "https://query1.finance.yahoo.com/v1/finance/search?q=Apple&quotesCount=2&newsCount=0" \
            | jq '.quotes[0] | {symbol,exchDisp}' || true

      # 4) Parse feeds, categorize, dedupe, PRN exchange-filter via inventory, and send to Slack
      - name: Parse RSS Feeds and Post to Slack
        env:
          SLACK_WEBHOOK_GENERAL: ${{ secrets.SLACK_WEBHOOK_GENERAL }}
          SLACK_WEBHOOK_MEDICAL: ${{ secrets.SLACK_WEBHOOK_MEDICAL }}
          SLACK_WEBHOOK_CONTRACTS: ${{ secrets.SLACK_WEBHOOK_CONTRACTS }}
        run: |
          set -euo pipefail

          FEEDS=(
            "https://feed.businesswire.com/rss/home/?rss=G1QFDERJXkJeEFpRXEMGSQ5SVFJUGExaFEhaUlJDFUkQUhFUUFNdGEU="
            "https://www.globenewswire.com/rssfeed/exchange/Nasdaq,NYSE"
            "https://www.prnewswire.com/rss/all-news-releases-from-PR-newswire-news.rss"
          )
          PRN_FEED="https://www.prnewswire.com/rss/all-news-releases-from-PR-newswire-news.rss"

          declare -A WEBHOOKS=(
            [general]="${SLACK_WEBHOOK_GENERAL}"
            [medical]="${SLACK_WEBHOOK_MEDICAL}"
            [contracts]="${SLACK_WEBHOOK_CONTRACTS}"
          )

          PER_FEED_ITEMS=3
          WINDOW_SEC=600

          # Negative-cache TTL (days) for company lookups that didn't resolve to US exchanges
          NEG_TTL_DAYS=30
          NEG_TTL_SEC=$((NEG_TTL_DAYS * 24 * 3600))

          # Yahoo reliability hardening
          YF_UA='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'
          YF_LANG='en-US,en;q=0.9'

          CURRENT_TIME=$(date -u +%s)
          TEN_MINUTES_AGO=$((CURRENT_TIME - WINDOW_SEC))

          # --- prune rolling 10-min headline cache (format: "<epoch> <headline_hash>")
          awk -v cutoff="$TEN_MINUTES_AGO" '{
            if (NF >= 2) { epoch=$1; if (epoch ~ /^[0-9]+$/ && epoch >= cutoff) print $0; }
          }' .cache/last_10min_articles.txt > .cache/_pruned.tmp || true
          mv .cache/_pruned.tmp .cache/last_10min_articles.txt

          # ---------- Helpers ----------
          norm_company_name() {
            # Normalize & strip common suffixes/symbols
            echo "$1" \
              | sed 's/[â„¢Â®Â©]//g' \
              | sed 's/[â€œâ€"'\''()]//g' \
              | sed 's/&/ and /g' \
              | sed -E 's/\b(Inc|Incorporated|Corp|Corporation|Co|Company|Ltd|Limited|PLC|Group|Holdings?|LLC|LP|AG|S\.?A\.?)\b\.?,?//Ig' \
              | sed -E 's/ +/ /g; s/^ +| +$//g' \
              | tr '[:upper:]' '[:lower:]'
          }

          extract_company_from_title() {
            local t="$1"

            # 1) Prefer the part before common separators
            local pre
            pre=$(echo "$t" | sed -E 's/ â€” / - /g' | awk -F' - |:|\\|\\|' '{print $1}')
            [ -z "$pre" ] && pre="$t"

            # Keep spacing, keep case for heuristics
            pre=$(echo "$pre" | tr -s ' ')

            # If the very first token is ALL CAPS (e.g., AEWIN), treat that as company
            local first
            first=$(echo "$pre" | awk '{print $1}')
            if echo "$first" | grep -qE '^[A-Z0-9\-]{2,}$'; then
              echo "$first"
              return 0
            fi

            # Remove a leading verb as the second token (e.g., "Company announces ...")
            local VERBS='launches|announces|reports|unveils|introduces|reveals|confirms|completes|signs|wins|awarded|enters|partners|acquires|files|prices|lists|successfully|passes|secures|appoints|names|extends|expands'
            local cleaned
            cleaned=$(echo "$pre" | awk -v verbs="$VERBS" '{
              if (NF>=2) {
                second=$2
                cmd="echo \"" second "\" | tr \"[:upper:]\" \"[:lower:]\""
                cmd | getline second_lc; close(cmd)
                if (second_lc ~ "^(" verbs ")$") {
                  $2=""; sub(/^  +/,""); gsub(/  +/," ");
                }
              }
              print
            }')

            # Drop common product nouns if they immediately follow the company name
            local GENERIC_NOUNS='inverters|series|platform|solution|solutions|product|products|system|systems|software|hardware|services|technology|technologies|network|appliance|appliances|device|devices'
            cleaned=$(echo "$cleaned" | sed -E "s/\\b($GENERIC_NOUNS)\\b( |$)//I")

            # Limit to first 6 words
            cleaned=$(echo "$cleaned" | awk '{for(i=1;i<=NF && i<=6;i++) printf (i==1?$i:" "$i); print ""}')
            cleaned=$(echo "$cleaned" | sed -E 's/^ +| +$//g')

            [ -z "$cleaned" ] && cleaned="$first"
            echo "$cleaned"
          }

          lookup_company_exchange() {
            # $1: raw company guess (from title)
            local raw="$1"
            local key norm
            norm=$(norm_company_name "$raw")
            key="$norm"

            local now_epoch
            now_epoch=$(date -u +%s)

            # 1) Inventory check (with TTL for negative entries)
            if jq -e --arg k "$key" 'has($k)' .cache/company_index.json >/dev/null; then
              entry=$(jq -r --arg k "$key" '.[$k]' .cache/company_index.json)
              cached_ticker=$(echo "$entry" | jq -r '.ticker')
              cached_exchange=$(echo "$entry" | jq -r '.exchange')
              last_checked=$(echo "$entry" | jq -r '.last_checked')
              last_epoch=$(date -u -d "$last_checked" +%s 2>/dev/null || echo 0)

              if [ "$cached_ticker" != "null" ] && [ -n "$cached_ticker" ] && [ "$cached_ticker" != "" ]; then
                echo "$entry"
                return 0
              fi

              # negative/unknown: honor TTL
              age=$(( now_epoch - last_epoch ))
              if [ "$last_epoch" -gt 0 ] && [ "$age" -lt "$NEG_TTL_SEC" ]; then
                echo "$entry"
                return 0
              fi
            fi

            # 2) Remote search (Yahoo Finance) â€” add UA, retry/backoff, fallback
            local q url j ok symbol exchDisp
            q=$(printf '%s' "$raw" | sed 's/ /%20/g')

            # primary search with retries
            max_tries=3
            sleep_s=1
            j=""
            for try in $(seq 1 $max_tries); do
              j=$(curl -sL --max-time 8 \
                    -H "User-Agent: $YF_UA" \
                    -H "Accept-Language: $YF_LANG" \
                    "https://query1.finance.yahoo.com/v1/finance/search?q=$q&quotesCount=5&newsCount=0" || true)
              if echo "$j" | jq -e '.quotes | length >= 1' >/dev/null 2>&1; then
                break
              fi
              sleep "$sleep_s"; sleep_s=$(( sleep_s * 2 ))
            done

            # fallback lookup if primary was empty/flaky
            if ! echo "$j" | jq -e '.quotes | length >= 1' >/dev/null 2>&1; then
              j=$(curl -sL --max-time 8 \
                    -H "User-Agent: $YF_UA" \
                    -H "Accept-Language: $YF_LANG" \
                    "https://query1.finance.yahoo.com/v1/finance/lookup?formatted=true&lang=en-US&region=US&query=$q&type=equity&count=5" || true)
            fi

            ok=false
            symbol=""
            exchDisp=""

            if [ -n "$j" ]; then
              mapfile -t lines < <(echo "$j" | jq -cr '.quotes[]? | select(.quoteType=="EQUITY") | {symbol:.symbol, exchDisp:.exchDisp}')
              for line in "${lines[@]:-}"; do
                symbol=$(echo "$line" | jq -r '.symbol')
                exchDisp=$(echo "$line" | jq -r '.exchDisp')
                if echo "$exchDisp" | grep -Ei '^(NASDAQ|NYSE|NYSE American|NYSE Arca)$' >/dev/null; then
                  ok=true; break
                fi
              done
            fi

            if [ "$ok" = true ]; then
              tmp=$(mktemp)
              jq --arg k "$key" \
                 --arg sym "$symbol" \
                 --arg exd "$exchDisp" \
                 --arg t "$(date -u +%FT%TZ)" \
                 '.[$k] = {ticker:$sym, exchange:$exd, last_checked:$t}' \
                 .cache/company_index.json > "$tmp" && mv "$tmp" .cache/company_index.json
              echo "{\"ticker\":\"$symbol\",\"exchange\":\"$exchDisp\"}"
              return 0
            else
              tmp=$(mktemp)
              jq --arg k "$key" --arg t "$(date -u +%FT%TZ)" \
                 '.[$k] = {ticker:null, exchange:null, last_checked:$t}' \
                 .cache/company_index.json > "$tmp" && mv "$tmp" .cache/company_index.json
              echo '{"ticker":null,"exchange":null}'
              return 0
            fi
          }

          NEW_ARTICLE_FOUND=false

          for FEED_URL in "${FEEDS[@]}"; do
            echo "ðŸ” Checking Feed: $FEED_URL"
            XML=$(curl -sL "$FEED_URL" || true)
            [ -z "$XML" ] && { echo "âš ï¸  Empty response from feed."; continue; }

            for i in $(seq 1 "$PER_FEED_ITEMS"); do
              LINK=$(
                echo "$XML" | xmllint --xpath "string((//item|//entry)[$i]/link | (//item|//entry)[$i]/link/@href)" - 2>/dev/null
              )
              TITLE=$(
                echo "$XML" | xmllint --xpath "string((//item|//entry)[$i]/title)" - 2>/dev/null
              )
              DESCRIPTION=$(
                echo "$XML" | xmllint --xpath "string((//item|//entry)[$i]/description | //entry[$i]/summary)" - 2>/dev/null
              )
              PUBDATE=$(
                echo "$XML" | xmllint --xpath "string((//item|//entry)[$i]/pubDate | //entry[$i]/updated)" - 2>/dev/null
              )

              [ -z "$LINK" ] && continue
              [ -z "$TITLE" ] && TITLE="(No Title)"
              [ -z "$DESCRIPTION" ] && DESCRIPTION="(No summary available)"
              [ -z "$PUBDATE" ] && PUBDATE=""

              DESCRIPTION=$(echo "$DESCRIPTION" | w3m -dump -T text/html | tr -s '[:space:]' ' ' | sed 's/^ *//;s/ *$//' | head -c 300)
              CONTENT="$TITLE $DESCRIPTION"
              CONTENT_LOWER=$(echo "$CONTENT" | tr '[:upper:]' '[:lower:]')

              # Robust date parsing
              ARTICLE_TIME=$(date -d "$PUBDATE" +%s 2>/dev/null || echo 0)
              if [ "$ARTICLE_TIME" -eq 0 ] || [ "$ARTICLE_TIME" -lt "$TEN_MINUTES_AGO" ]; then
                echo "â±ï¸ Skipping (invalid/old date): $TITLE"
                continue
              fi

              # Short-term dedupe (10-min window) â€” by headline hash
              HEADLINE_HASH=$(echo -n "$TITLE" | md5sum | cut -d ' ' -f1)
              if awk '{print $2}' .cache/last_10min_articles.txt | grep -qx "$HEADLINE_HASH"; then
                echo "ðŸ” Skipping duplicate headline (10-min window): $TITLE"
                continue
              fi

              # Global dedupe across runs â€” by link hash and headline hash
              LINK_HASH=$(echo -n "$LINK" | md5sum | cut -d ' ' -f1)
              if grep -qx "$LINK_HASH" .cache/sent_hashes.txt; then
                echo "ðŸ§­ Already sent (by link): $TITLE"
                continue
              fi
              if grep -qx "$HEADLINE_HASH" .cache/sent_hashes.txt; then
                echo "ðŸ§­ Already sent (by headline): $TITLE"
                continue
              fi

              # --- US-listing filter for PR Newswire ONLY (via inventory+lookup)
              if [ "$FEED_URL" = "$PRN_FEED" ]; then
                company_guess=$(extract_company_from_title "$TITLE")
                info_json=$(lookup_company_exchange "$company_guess")
                exch=$(echo "$info_json" | jq -r '.exchange // empty')
                ticker=$(echo "$info_json" | jq -r '.ticker // empty')

                if ! echo "$exch" | grep -Ei '^(NASDAQ|NYSE|NYSE American|NYSE Arca)$' >/dev/null; then
                  echo "ðŸ‡ºðŸ‡¸â›” Skipping PRN item not resolved to US exchange: $TITLE (company: $company_guess, exchange: ${exch:-unknown})"
                  continue
                fi
                echo "ðŸ‡ºðŸ‡¸âœ… PRN item resolved to US exchange: $TITLE (company: $company_guess, exchange: $exch, ticker: ${ticker:-N/A})"
              fi

              # Categorize
              if echo "$CONTENT_LOWER" | grep -qE "positive topline|positive trial results|collaboration|funding"; then
                CATEGORY="medical"
              elif echo "$CONTENT_LOWER" | grep -qE "contract awarded|acquisition|investment|deal|strategic partnership"; then
                CATEGORY="contracts"
              else
                CATEGORY="general"
              fi

              # Sentiment + naive recommendation
              if echo "$CONTENT_LOWER" | grep -qE "record|growth|surge|positive|expands|secured|approved|funding|investment|invests|acquire|acquired|launch|topline|announces|partnership"; then
                SENTIMENT=":large_green_circle: [POSITIVE]"
                RECOMMENDATION="BUY"
              elif echo "$CONTENT_LOWER" | grep -qE "layoff|lawsuit|delay|loss|drop|decline|recall|fails|resigns|investigation"; then
                SENTIMENT=":red_circle: [NEGATIVE]"
                RECOMMENDATION="SELL"
              else
                SENTIMENT=":white_circle: [NEUTRAL]"
                RECOMMENDATION="HOLD"
              fi

              PAYLOAD=$(jq -n \
                --arg title "$TITLE" \
                --arg desc "$DESCRIPTION" \
                --arg link "$LINK" \
                --arg sentiment "$SENTIMENT" \
                --arg pubdate "${PUBDATE:-Unknown}" \
                --arg rec "$RECOMMENDATION" \
                '{
                  blocks: [
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":bell: *New Article*\n\($sentiment) *\($title)*" } },
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":page_facing_up: \($desc)" } },
                    { "type": "context", "elements": [
                        { "type": "mrkdwn", "text": ":link: <\($link)|Read more>" },
                        { "type": "mrkdwn", "text": ":clock3: Posted: \($pubdate)" },
                        { "type": "mrkdwn", "text": ":mag: Suggestion: *\($rec)*" }
                      ]
                    }
                  ]
                }')

              echo "ðŸ“¤ Sending to Slack ($CATEGORY)â€¦"
              curl -s -X POST "${WEBHOOKS[$CATEGORY]}" \
                -H 'Content-type: application/json' \
                --data "$PAYLOAD" >/dev/null || true

              # Mark dedupe: global and 10-min caches
              echo "$LINK_HASH" >> .cache/sent_hashes.txt
              echo "$HEADLINE_HASH" >> .cache/sent_hashes.txt
              echo "$CURRENT_TIME $HEADLINE_HASH" >> .cache/last_10min_articles.txt

              NEW_ARTICLE_FOUND=true
            done
          done

          if [ "$NEW_ARTICLE_FOUND" = false ]; then
            echo "âœ… No new articles in the last 10 minutes."
          fi

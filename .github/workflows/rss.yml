name: Categorized RSS Alerts with Sentiment

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "*/10 * * * *"

concurrency:
  group: rss-alerts
  cancel-in-progress: false

jobs:
  notify:
    runs-on: ubuntu-latest

    steps:
      - name: Set up environment
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-utils w3m jq curl coreutils gawk python3

      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: .cache
          key: rss-cache-${{ runner.os }}

      - name: Ensure cache files
        run: |
          mkdir -p .cache
          [ -f .cache/sent_link_hashes.txt ] || : > .cache/sent_link_hashes.txt
          [ -f .cache/sent_headline_hashes.txt ] || : > .cache/sent_headline_hashes.txt
          [ -f .cache/last_10min_articles.txt ] || : > .cache/last_10min_articles.txt
          [ -f .cache/symbol_db.json ] || echo '{}' > .cache/symbol_db.json
          [ -f .cache/symbols.sha256 ] || : > .cache/symbols.sha256
          [ -f .cache/negative_prn.json ] || echo '{}' > .cache/negative_prn.json
          [ -f .cache/important_keys.json ] || echo '{}' > .cache/important_keys.json
          [ -f .cache/important.sha256 ] || : > .cache/important.sha256

      - name: Build symbol lookup (from symbols.csv via Python)
        shell: bash
        run: |
          set -euo pipefail
          INPUT_FILE="data/symbols.csv"
          if [ ! -f "$INPUT_FILE" ]; then
            echo "‚ùå Missing $INPUT_FILE"
            exit 1
          fi

          NEW_HASH="$(sha256sum "$INPUT_FILE" | awk '{print $1}')"
          OLD_HASH="$(cat .cache/symbols.sha256 || true)"

          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -s .cache/symbol_db.json ] && jq -e . .cache/symbol_db.json >/dev/null 2>&1; then
            echo "üü¢ symbols.csv unchanged; keys: $(jq 'keys|length' .cache/symbol_db.json || echo '0')"
            exit 0
          fi

          echo "üîÑ Rebuilding lookup from symbols.csv using Python‚Ä¶"
          python3 - <<'PY' > .cache/symbol_db.json
import csv
import json
import sys
import re
from pathlib import Path

inp = Path("data/symbols.csv")
raw = inp.read_bytes()
if raw.startswith(b'\xef\xbb\xbf'):  # Handle BOM
    raw = raw[3:]
text = raw.decode('utf-8', errors='replace')

# Detect delimiter
sample = "\n".join(text.splitlines()[:5])
dialect = csv.Sniffer().sniff(sample, delimiters="\t,;")
reader = csv.reader(text.splitlines(), dialect)

rows = list(reader)
if not rows:
    print("{}"); sys.exit(0)

header = rows[0]
lower = [h.strip().lower() for h in header]
has_header = ("symbol" in lower and ("name" in lower or "company" in lower or "company name" in lower))
start = 1 if has_header else 0

def idx_of(cands):
    for c in cands:
        if c in lower: return lower.index(c)
    return None

sym_idx = idx_of(["symbol", "ticker", "ticker symbol"]) or 0
name_idx = idx_of(["name", "company", "company name", "security name"]) or 1

db = {}
def norm_key(s):
    s = re.sub(r"\s+", " ", s.strip().lower())
    return s

for r in rows[start:]:
    if len(r) <= max(sym_idx, name_idx): continue
    sym, name = r[sym_idx].strip(), r[name_idx].strip()
    if not sym or not name: continue
    toks = [t for t in name.split() if t]
    for n in (1, 2, 3, 4):
        if len(toks) < n: break
        pref = " ".join(toks[:n])
        k = norm_key(pref)
        if k and k not in db: db[k] = {"ticker": sym}
        kand = norm_key(k.replace("+", " and ").replace("&", " and "))
        if kand != k and kand and kand not in db: db[kand] = {"ticker": sym}

print(json.dumps(db, ensure_ascii=False))
PY
          if ! jq -e . .cache/symbol_db.json >/dev/null 2>&1; then
            echo "‚ùå Generated symbol_db.json is invalid"
            exit 1
          fi
          echo "$NEW_HASH" > .cache/symbols.sha256
          echo "‚úÖ Lookup built (keys: $(jq 'keys|length' .cache/symbol_db.json))"
          jq -r 'to_entries | .[0:5] | .[] | "\(.key) => \(.value.ticker)"' .cache/symbol_db.json || true

      - name: Build important company lookup (from TSV)
        shell: bash
        run: |
          set -euo pipefail
          INPUT_FILE="data/important_companies.tsv"
          if [ ! -f "$INPUT_FILE" ]; then
            echo "{}" > .cache/important_keys.json
            echo "" > .cache/important.sha256
            echo "‚ö†Ô∏è No important_companies.tsv found."
            exit 0
          fi

          NEW_HASH="$(sha256sum "$INPUT_FILE" | awk '{print $1}')"
          OLD_HASH="$(cat .cache/important.sha256 || true)"

          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -s .cache/important_keys.json ] && jq -e . .cache/important_keys.json >/dev/null 2>&1; then
            echo "üü¢ important_companies.tsv unchanged; keys: $(jq 'keys|length' .cache/important_keys.json || echo '0')"
            exit 0
          fi

          echo "üîÑ Building important company prefixes (1..4 words)‚Ä¶"
          python3 - <<'PY' > .cache/important_keys.json
import csv
import json
import sys
import re
from pathlib import Path

inp = Path("data/important_companies.tsv")
if not inp.exists():
    print("{}"); sys.exit(0)

raw = inp.read_bytes()
if raw.startswith(b'\xef\xbb\xbf'):
    raw = raw[3:]
text = raw.decode('utf-8', errors='replace')

sample = "\n".join(text.splitlines()[:5])
dialect = csv.Sniffer().sniff(sample, delimiters="\t,;")
reader = csv.reader(text.splitlines(), dialect)

rows = list(reader)
if not rows:
    print("{}"); sys.exit(0)

header = rows[0]
lower = [h.strip().lower() for h in header]
has_header = any(h in lower for h in ["company", "company name", "name"])
start = 1 if has_header else 0

db = {}
def norm_key(s):
    s = re.sub(r"\s+", " ", s.strip().lower())
    return s

for r in rows[start:]:
    if not r or not r[0].strip(): continue
    name = r[0].strip()
    ticker = r[1].strip() if len(r) > 1 and r[1].strip() else None
    toks = [t for t in name.split() if t]
    for n in (1, 2, 3, 4):
        if len(toks) < n: break
        pref = " ".join(toks[:n])
        k = norm_key(pref)
        if k and k not in db: db[k] = {"ticker": ticker}
        kand = norm_key(k.replace("+", " and ").replace("&", " and "))
        if kand != k and kand and kand not in db: db[kand] = {"ticker": ticker}

print(json.dumps(db, ensure_ascii=False))
PY
          if ! jq -e . .cache/important_keys.json >/dev/null 2>&1; then
            echo "‚ùå Generated important_keys.json is invalid"
            exit 1
          fi
          echo "$NEW_HASH" > .cache/important.sha256
          echo "‚úÖ Important company keys: $(jq 'keys|length' .cache/important_keys.json)"
          jq -r 'to_entries | .[0:5] | .[] | "\(.key) => \(.value.ticker // "null")"' .cache/important_keys.json || true

      - name: Parse RSS Feeds and Post to Slack
        env:
          SLACK_WEBHOOK_GENERAL: ${{ secrets.SLACK_WEBHOOK_GENERAL }}
          SLACK_WEBHOOK_MEDICAL: ${{ secrets.SLACK_WEBHOOK_MEDICAL }}
          SLACK_WEBHOOK_CONTRACTS: ${{ secrets.SLACK_WEBHOOK_CONTRACTS }}
          SLACK_WEBHOOK_EARNINGS: ${{ secrets.SLACK_WEBHOOK_EARNINGS }}
          SLACK_WEBHOOK_IMP: ${{ secrets.SLACK_WEBHOOK_IMP }}
          DEBUG: false
        shell: bash
        run: |
          set -euo pipefail

          FEEDS=(
            "https://feed.businesswire.com/rss/home/?rss=G1QFDERJXkJeEFpRXEMGSQ5SVFJUGExaFEhaUlJDFUkQUhFUUFNdGEU="
            "https://www.globenewswire.com/rssfeed/exchange/Nasdaq,NYSE"
            "https://www.prnewswire.com/rss/news-releases-list.rss"
          )
          PRN_FEED="https://www.prnewswire.com/rss/news-releases-list.rss"

          declare -A WEBHOOKS=(
            [general]="${SLACK_WEBHOOK_GENERAL}"
            [medical]="${SLACK_WEBHOOK_MEDICAL}"
            [contracts]="${SLACK_WEBHOOK_CONTRACTS}"
            [earnings]="${SLACK_WEBHOOK_EARNINGS}"
            [important]="${SLACK_WEBHOOK_IMP}"
          )

          EARNINGS_REGEX='(^|[[:space:][:punct:]])(reports?|announces?) ([[:alpha:]]+ )?(first|second|third|fourth|q[1-4]) (quarter|results|earnings)|(^|[[:space:][:punct:]])fiscal [0-9]{4} (results|earnings)|(^|[[:space:][:punct:]])earnings (results|release|call)|(^|[[:space:][:punct:]])q[1-4] (results|earnings)|(^|[[:space:][:punct:]])eps([[:space:][:punct:]]|$)'
          MEDICAL_KEYWORDS="positive topline|positive trial results|collaboration|funding"
          CONTRACTS_KEYWORDS="contract awarded|acquisition|investment|deal|strategic partnership"
          POSITIVE_KEYWORDS="record|growth|surge|positive|expands|secured|approved|funding|investment|invests|acquire|acquired|launch|topline|announces|partnership"
          NEGATIVE_KEYWORDS="layoff|lawsuit|delay|loss|drop|decline|recall|fails|resigns|investigation"

          WINDOW_SEC=600
          CURRENT_TIME=$(date -u +%s)
          CUTOFF=$((CURRENT_TIME - WINDOW_SEC))

          awk -v cutoff="$CUTOFF" '$1 >= cutoff {print}' .cache/last_10min_articles.txt > .cache/_p.tmp || true
          mv .cache/_p.tmp .cache/last_10min_articles.txt

          norm_key() { echo "$1" | tr '[:upper:]' '[:lower:]' | sed -E 's/ +/ /g; s/^ +| +$//g'; }

          lookup_symbol() {
            local key="$1"
            jq -cr --arg k "$key" 'if has($k) then .[$k] else null end' .cache/symbol_db.json
          }

          extract_chunk_before_verb() {
            local title="$1"
            local pre=$(echo "$title" | sed -E 's/ ‚Äî / - /g' | awk -F' - |:|\\|\\|' '{print $1}')
            [ -z "$pre" ] && pre="$title"
            pre=$(echo "$pre" | sed 's/[‚Äú‚Äù"()‚Ñ¢¬Æ¬©]//g' | tr -s ' ')
            awk '{
              n=NF; chunk=""
              verbs="reports report announces announce launches launch unveils introduces reveals confirms completes signs wins awarded enters partners acquires files prices lists successfully passes secures appoints names extends expands approves approved"
              split(verbs,V," ")
              for(i=1;i<=n;i++){
                wl=tolower($i); for(v in V) if(wl==V[v]) exit
                if($i ~ /^[A-Z][A-Za-z0-9.+&-]*$/ || $i ~ /^[A-Z0-9-]{2,}$/){
                  chunk = (length(chunk)?chunk" ":"") $i
                } else if(length(chunk)>0) exit
              }
              print chunk
            }' <<<"$pre" | sed -E 's/^ +| +$//g'
          }

          is_important() {
            local title="$1"
            local tnorm=$(norm_key "$title")
            local cand=$(norm_key "$(extract_chunk_before_verb "$title")")
            jq -e --arg t "$tnorm" --arg c "$cand" 'has($t) or has($c)' .cache/important_keys.json >/dev/null
          }

          is_negative_cached() { jq -e --arg k "$1" 'has($k)' .cache/negative_prn.json >/dev/null 2>&1; }
          mark_negative() {
            local key="$1"
            tmp=$(mktemp)
            jq --arg k "$key" --arg t "$(date -u +%FT%TZ)" \
               '.[$k] = {last_checked:$t}' .cache/negative_prn.json > "$tmp" && mv "$tmp" .cache/negative_prn.json
          }
          unmark_negative() {
            local key="$1"
            tmp=$(mktemp)
            jq --arg k "$key" 'del(.[$k])' .cache/negative_prn.json > "$tmp" && mv "$tmp" .cache/negative_prn.json
          }

          NEW_ARTICLE_FOUND=false

          for FEED_URL in "${FEEDS[@]}"; do
            echo "üîç Checking Feed: ${FEED_URL##*/}"
            XML_FILE=$(mktemp)
            curl -sL --max-time 30 --retry 2 "$FEED_URL" -o "$XML_FILE" || { echo "‚ö†Ô∏è Failed to fetch $FEED_URL"; rm -f "$XML_FILE"; continue; }
            [ -s "$XML_FILE" ] || { echo "‚ö†Ô∏è Empty feed: $FEED_URL"; rm -f "$XML_FILE"; continue; }

            ITEMS=$(xmllint --xpath 'count(//item | //entry)' "$XML_FILE" 2>/dev/null || echo 0)
            ITEMS=${ITEMS%.*}
            echo "üì• Processing $ITEMS items"

            for i in $(seq 1 "$ITEMS"); do
              LINK=$(xmllint --xpath "string((//item|//entry)[$i]/link | (//item|//entry)[$i]/link/@href)" "$XML_FILE" 2>/dev/null || true)
              TITLE=$(xmllint --xpath "string((//item|//entry)[$i]/title)" "$XML_FILE" 2>/dev/null || true)
              DESCRIPTION=$(xmllint --xpath "string((//item|//entry)[$i]/description | //entry[$i]/summary)" "$XML_FILE" 2>/dev/null || true)
              PUBDATE=$(xmllint --xpath "string((//item|//entry)[$i]/pubDate | //entry[$i]/updated)" "$XML_FILE" 2>/dev/null || true)
              [ -z "$LINK" ] && { ${DEBUG} && echo "Skipping (no link): item $i"; continue; }
              [ -z "$TITLE" ] && TITLE="(No Title)"
              [ -z "$DESCRIPTION" ] && DESCRIPTION="(No summary available)"
              [ -z "$PUBDATE" ] && PUBDATE="Unknown"

              DESCRIPTION=$(echo "$DESCRIPTION" | w3m -dump -T text/html | tr -s '[:space:]' ' ' | sed 's/^ *//;s/ *$//' | head -c 300)
              CONTENT="$TITLE $DESCRIPTION"
              CONTENT_LOWER=$(norm_key "$CONTENT")

              ARTICLE_TIME=$(date -d "$PUBDATE" +%s 2>/dev/null || echo 0)
              [ "$ARTICLE_TIME" -gt 0 ] || { ${DEBUG} && echo "Skipping (invalid date): $TITLE"; continue; }
              [ "$ARTICLE_TIME" -ge "$CUTOFF" ] || { ${DEBUG} && echo "Skipping (old): $TITLE"; continue; }

              HEAD_HASH=$(echo -n "$TITLE" | sha256sum | cut -d ' ' -f1)
              LINK_HASH=$(echo -n "$LINK" | sha256sum | cut -d ' ' -f1)
              if grep -qx "$HEAD_HASH" <(awk '{print $2}' .cache/last_10min_articles.txt); then
                ${DEBUG} && echo "Skipping (recent dup): $TITLE"
                continue
              fi
              if grep -qx "$LINK_HASH" .cache/sent_link_hashes.txt || grep -qx "$HEAD_HASH" .cache/sent_headline_hashes.txt; then
                ${DEBUG} && echo "Skipping (sent dup): $TITLE"
                continue
              fi

              if [ "$FEED_URL" = "$PRN_FEED" ]; then
                matched=false
                cand_full=$(extract_chunk_before_verb "$TITLE")
                if [ -n "$cand_full" ]; then
                  read -r -a CAND_TOKS <<<"$cand_full"
                  for n in 1 2 3 4; do
                    [ ${#CAND_TOKS[@]} -ge $n ] || break
                    cand_prefix=$(norm_key "${let me know if you need further refinements or have specific requirements to add!CAND_TOKS[*]:0:$n}")
                    if is_negative_cached "$cand_prefix"; then continue; fi
                    info=$(lookup_symbol "$cand_prefix")
                    ticker=$(jq -r '.ticker // empty' <<<"$info")
                    if [ -n "$ticker" ]; then
                      matched=true
                      unmark_negative "$cand_prefix" || true
                      break
                    fi
                  done
                fi
                if [ "$matched" = false ]; then
                  mark_negative "$(norm_key "${cand_full:-}")"
                  ${DEBUG} && echo "Skipping (non-US PRN): $TITLE"
                  continue
                fi
              fi

              if echo "$CONTENT_LOWER" | grep -qE "$EARNINGS_REGEX"; then
                CATEGORY="earnings"
              elif echo "$CONTENT_LOWER" | grep -qE "$MEDICAL_KEYWORDS"; then
                CATEGORY="medical"
              elif echo "$CONTENT_LOWER" | grep -qE "$CONTRACTS_KEYWORDS"; then
                CATEGORY="contracts"
              else
                CATEGORY="general"
              fi

              if echo "$CONTENT_LOWER" | grep -qE "$POSITIVE_KEYWORDS"; then
                SENTIMENT=":large_green_circle: [POSITIVE]"; RECOMMENDATION="BUY"
              elif echo "$CONTENT_LOWER" | grep -qE "$NEGATIVE_KEYWORDS"; then
                SENTIMENT=":red_circle: [NEGATIVE]"; RECOMMENDATION="SELL"
              else
                SENTIMENT=":white_circle: [NEUTRAL]"; RECOMMENDATION="HOLD"
              fi

              SEND_TO_IMP=false
              if is_important "$TITLE"; then
                SEND_TO_IMP=true
              fi

              PAYLOAD=$(jq -n \
                --arg title "$TITLE" \
                --arg desc "$DESCRIPTION" \
                --arg link "$LINK" \
                --arg sentiment "$SENTIMENT" \
                --arg pubdate "$PUBDATE" \
                --arg rec "$RECOMMENDATION" \
                '{
                  blocks: [
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":bell: *New Article*\n\($sentiment) *\($title)*" } },
                    { "type": "section", "text": { "type": "mrkdwn", "text": ":page_facing_up: \($desc)" } },
                    { "type": "context", "elements": [
                        { "type": "mrkdwn", "text": ":link: <\($link)|Read more>" },
                        { "type": "mrkdwn", "text": ":clock3: Posted: \($pubdate)" },
                        { "type": "mrkdwn", "text": ":mag: Suggestion: *\($rec)*" }
                      ]
                    }
                  ]
                }')

              if [ -n "${WEBHOOKS[$CATEGORY]}" ]; then
                curl -s -X POST "${WEBHOOKS[$CATEGORY]}" -H 'Content-type: application/json' --data "$PAYLOAD" >/dev/null || echo "‚ö†Ô∏è Failed to send to $CATEGORY webhook"
              fi

              if [ "$SEND_TO_IMP" = true ] && [ -n "${WEBHOOKS[important]}" ]; then
                curl -s -X POST "${WEBHOOKS[important]}" -H 'Content-type: application/json' --data "$PAYLOAD" >/dev/null || echo "‚ö†Ô∏è Failed to send to important webhook"
              fi

              echo "$HEAD_HASH" >> .cache/sent_headline_hashes.txt
              echo "$LINK_HASH" >> .cache/sent_link_hashes.txt
              echo "$ARTICLE_TIME $HEAD_HASH" >> .cache/last_10min_articles.txt
              NEW_ARTICLE_FOUND=true
              ${DEBUG} && echo "‚úÖ Sent: $TITLE (category: $CATEGORY${SEND_TO_IMP:+, important})"
            done

            rm -f "$XML_FILE"
          done

          if [ "$NEW_ARTICLE_FOUND" = false ]; then
            echo "‚úÖ No new articles in last 10 minutes."
          fi
